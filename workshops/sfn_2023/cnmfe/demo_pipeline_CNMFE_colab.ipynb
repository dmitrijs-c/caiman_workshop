{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BJoVp81_sdz"
   },
   "source": [
    "This notebook is a Colab-friendly update to CaImAn's demo notebook (demo_pipeline_CNMFE.ipynb), and has been adapted for use at the 2023 SFN Satellite Workshop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jXkvua__d9Aq"
   },
   "source": [
    "# CNMF-E demo pipeline: Intro\n",
    "This notebook demonstrates how to use Caiman for processing 1p microendoscopic data. It shows how to use Caiman for the following steps:\n",
    "\n",
    "![cnmfe pipeline full](https://raw.githubusercontent.com/EricThomson/image_sandbox/main/images/full_cnmfe_workflow.jpg)\n",
    "\n",
    "1. Apply the NoRMCorre (nonrigid motion correction) algorithm for motion correction.\n",
    "2. Apply the constrained nonnegative matrix factorization endoscopic (CNMF-E) source separation algorithm to extract an initial estimate of neuronal spatial footprint and calcium traces.\n",
    "3. Apply quality control metrics to evaluate the initial estimates, and narrow down to the final set of estimates.\n",
    "\n",
    "Some tools for visualization of movies and results are also included.\n",
    "\n",
    "> This demo follows a similar pattern to the CNMF demo in `demo_pipeline.ipynb`. It includes less explanation except where there are important differences. If you want to get a more explanation-heavy picture of the fundamentals, we suggest starting with `demo_pipeline.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bh2kN0S_6So5"
   },
   "source": [
    "## Installation\n",
    "This can take a few minutes we are installing Caiman from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sr-azHZIOYQo"
   },
   "outputs": [],
   "source": [
    "# Clone CaImAn from Github\n",
    "!git clone -b colab https://github.com/EricThomson/CaImAn.git\n",
    "%cd /content/CaImAn\n",
    "\n",
    "# Install relevant packages\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "# Install CaImAn package\n",
    "!pip install -e .\n",
    "!pip install mpld3\n",
    "\n",
    "# Install CaImAn manager for relevant datasets\n",
    "!python caimanmanager.py install --inplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbAEyPLbd9At"
   },
   "source": [
    "### Imports and general setup\n",
    "We first need to import the Python libraries we will use in the rest of the notebook and tweak some general settings. Don't worry about these details now, we will explain the important things when they come up. Note in the following, we import `caiman` as `cm`, so when you see `cm` in the rest of the notebook, it just means you are using something from the Caiman package.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IyWbhQUtd9At"
   },
   "outputs": [],
   "source": [
    "import bokeh.plotting as bpl\n",
    "import cv2\n",
    "import datetime\n",
    "import glob\n",
    "import holoviews as hv\n",
    "import imageio\n",
    "from IPython import get_ipython\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import mpld3\n",
    "import numpy as np\n",
    "import os\n",
    "import psutil\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    cv2.setNumThreads(0)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    if __IPYTHON__:\n",
    "        get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "        get_ipython().run_line_magic('autoreload', '2')\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "import caiman as cm\n",
    "from caiman.motion_correction import MotionCorrect\n",
    "from caiman.source_extraction.cnmf import cnmf as cnmf\n",
    "from caiman.source_extraction.cnmf import params as params\n",
    "from caiman.utils.utils import download_demo\n",
    "from caiman.utils.visualization import plot_contours, nb_view_patches, nb_plot_contour\n",
    "from caiman.summary_images import local_correlations_movie_offline\n",
    "import psutil\n",
    "from scipy.ndimage import center_of_mass\n",
    "from IPython.display import display, clear_output\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "import bokeh\n",
    "import bokeh.plotting as bpl\n",
    "from bokeh.io import output_notebook\n",
    "import holoviews as hv\n",
    "output_notebook()\n",
    "hv.extension('bokeh')\n",
    "\n",
    "%env HV_DOC_HTML=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJx8SftaRqPZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Go17YYgHd9At"
   },
   "source": [
    "Continuing with our basic setup, we will set up a logger, and also set some environment variables in case that wasn't done already in your shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vBW0IWVPd9At"
   },
   "outputs": [],
   "source": [
    "# set up logging\n",
    "logging.basicConfig(format=\"{asctime} - {levelname} - [{filename} {funcName}() {lineno}] - pid {process} - {message}\",\n",
    "                    filename=None,\n",
    "                    level=logging.WARNING, style=\"{\") #logging level can be DEBUG, INFO, WARNING, ERROR, CRITICAL\n",
    "\n",
    "# set env variables\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJ01yrm0d9Au"
   },
   "source": [
    "# Select file(s) to be processed\n",
    "Here, we analyze the data in `data_endoscope.tif`. The `download_demo` function will download the  file for you and return the complete path to the file which will be stored in your `caiman_data` directory. If you adapt this demo for your data make sure to pass the complete path to your file.\n",
    "\n",
    "Note that the memory requirement of the CNMF-E algorithm are much higher compared to the standard CNMF algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bksDwMKld9Av"
   },
   "outputs": [],
   "source": [
    "movie_path = download_demo('data_endoscope.tif')\n",
    "print(f\"Original movie for demo is in {movie_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LP5NBTLAd9Av"
   },
   "source": [
    "## Load and visualize raw data\n",
    "Using colab workarounds for viewing raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8lVIf7_V8jTW"
   },
   "outputs": [],
   "source": [
    "# in colab  -- 40 sec runtime\n",
    "# load original movie\n",
    "# display movie\n",
    "movie_orig = cm.load(movie_path)\n",
    "display_movie = True\n",
    "if display_movie:\n",
    "    ds_ratio = 0.2\n",
    "    moviehandle = movie_orig.resize(1, 1, ds_ratio)\n",
    "    min_, max_ = np.percentile(moviehandle, 0.0001), np.percentile(moviehandle, 99.9999)\n",
    "    moviehandle = np.array((moviehandle-min_)/(max_-min_)*255,dtype='uint8')\n",
    "    imageio.mimwrite('/root/caiman_data/demo_movie.mp4', moviehandle, fps = 10,  quality=7)\n",
    "    mp4 = open('/root/caiman_data/demo_movie.mp4','rb').read()\n",
    "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "    display(HTML(\"\"\"\n",
    "    <video width=400 controls>\n",
    "          <source src=\"%s\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    \"\"\" % data_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uT-1le8lK-kB"
   },
   "source": [
    "# Set up a cluster\n",
    "To enable parallel computing we will set up a local cluster. The resulting variable `cluster` contains the pool of processors (CPUs) that will be used in later steps. If you use `dview=cluster` in later steps, then parallel processing will be used. If you use `dview=None` then no parallel processing will be used. The `num_processors_to_use` variable determines how many CPU dores you will use (when set to `None` it goes to the default of one less than the number available).\n",
    "\n",
    "Note for free colab we use the max number available (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HxNLvXalLB62"
   },
   "outputs": [],
   "source": [
    "print(f\"You have {psutil.cpu_count()} CPUs available in your current environment\")\n",
    "num_processors_to_use = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEQGTK39LNUP"
   },
   "source": [
    "Set up a cluster of processors. If one has already been set up (the `cluster` variable is already in your namespace), then that cluster will be closed and a new one created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "THEzlhebLN_U"
   },
   "outputs": [],
   "source": [
    "#%% start a cluster for parallel processing (if a cluster already exists it will be closed and a new session will be opened)\n",
    "if 'cluster' in locals():  # 'locals' contains list of current local variables\n",
    "    print('Closing previous cluster')\n",
    "    cm.stop_server(dview=cluster)\n",
    "print(\"Setting up new cluster\")\n",
    "_, cluster, n_processes = cm.cluster.setup_cluster(backend='multiprocessing',\n",
    "                                                 n_processes=num_processors_to_use,\n",
    "                                                 ignore_preexisting=False)\n",
    "print(f\"Successfully set up cluster with {n_processes} processes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hfG787_Pd9Ay"
   },
   "source": [
    "# Set up some parameters\n",
    "We first set some parameters related to the data and motion correction and create a `params` object. We'll modify this parameter object later on with settings for source extraction. You can also set all the parameters at once as demonstrated in the `demo_pipeline.ipynb` notebook.\n",
    "\n",
    "Note here we are setting `pw_rigid` to `False` as our data seems to mainly contain large-scale translational motion. We can always redo this later if it turns out to be a mistake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aNMUg02_d9Ay"
   },
   "outputs": [],
   "source": [
    "# dataset dependent parameters\n",
    "frate = 10                       # movie frame rate\n",
    "decay_time = 0.4                 # length of a typical transient in seconds\n",
    "\n",
    "# motion correction parameters\n",
    "motion_correct = True    # flag for performing motion correction\n",
    "pw_rigid = False         # flag for performing piecewise-rigid motion correction (otherwise just rigid)\n",
    "gSig_filt = (3, 3)       # size of high pass spatial filtering, used in 1p data\n",
    "max_shifts = (5, 5)      # maximum allowed rigid shift\n",
    "strides = (48, 48)       # start a new patch for pw-rigid motion correction every x pixels\n",
    "overlaps = (24, 24)      # overlap between pathes (size of patch strides+overlaps)\n",
    "max_deviation_rigid = 3  # maximum deviation allowed for patch with respect to rigid shifts\n",
    "border_nan = 'copy'      # replicate values along the boundaries\n",
    "\n",
    "mc_dict = {\n",
    "    'fnames': movie_path,\n",
    "    'fr': frate,\n",
    "    'decay_time': decay_time,\n",
    "    'pw_rigid': pw_rigid,\n",
    "    'max_shifts': max_shifts,\n",
    "    'gSig_filt': gSig_filt,\n",
    "    'strides': strides,\n",
    "    'overlaps': overlaps,\n",
    "    'max_deviation_rigid': max_deviation_rigid,\n",
    "    'border_nan': border_nan\n",
    "}\n",
    "\n",
    "parameters = params.CNMFParams(params_dict=mc_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3g7trTxd9A0"
   },
   "source": [
    "# Motion Correction\n",
    "The background signal in micro-endoscopic data is very strong and makes motion correction challenging. As a first step the algorithm performs high pass spatial filtering with a Gaussian kernel to remove the bulk of the lower-frequency background activity and enhance spatial landmarks.\n",
    "\n",
    "The size of the kernel is given from the parameter `gSig_filt`. If this is left to the default value of `None` then no spatial filtering is performed (default option, used in 2p data for CNMF).\n",
    "\n",
    "After spatial filtering, the NoRMCorre algorithm is used to determine the motion in each frame. The inferred motion is then applied to the *original* data, so no information is lost before source separation. The motion corrected files are saved in memory mapped format. If no motion correction is performed (i.e., `motion_correct` was set to `False`), then the file gets directly memory mapped.\n",
    "\n",
    "The following also plots the discovered displacements in x- and y- in the movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HAmpSbMvd9A1"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if motion_correct:\n",
    "    # do motion correction rigid\n",
    "    mot_correct = MotionCorrect(movie_path, dview=cluster, **parameters.get_group('motion'))\n",
    "    mot_correct.motion_correct(save_movie=True)\n",
    "    fname_mc = mot_correct.fname_tot_els if pw_rigid else mot_correct.fname_tot_rig\n",
    "    if pw_rigid:\n",
    "        bord_px = np.ceil(np.maximum(np.max(np.abs(mot_correct.x_shifts_els)),\n",
    "                                     np.max(np.abs(mot_correct.y_shifts_els)))).astype(int)\n",
    "    else:\n",
    "        bord_px = np.ceil(np.max(np.abs(mot_correct.shifts_rig))).astype(int)\n",
    "        # Plot shifts\n",
    "        plt.plot(mot_correct.shifts_rig)  # % plot rigid shifts\n",
    "        plt.legend(['x shifts', 'y shifts'])\n",
    "        plt.xlabel('frames')\n",
    "        plt.ylabel('pixels')\n",
    "        plt.gcf().set_size_inches(6,3)\n",
    "\n",
    "    bord_px = 0 if border_nan == 'copy' else bord_px\n",
    "    fname_new = cm.save_memmap(fname_mc, base_name='memmap_', order='C',\n",
    "                               border_to_0=bord_px)\n",
    "else:  # if no motion correction just memory map the file\n",
    "    fname_new = cm.save_memmap(movie_path, base_name='memmap_',\n",
    "                               order='C', border_to_0=0, dview=dview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8enKVk7d9A1"
   },
   "source": [
    "Compare original (left) and motion corrected movie (right).\n",
    "\n",
    "You will probably notice they look pretty similar, as there wasn't much motion to begin with. You can see from the shift plot (plotted above) that the extracted shifts were all very small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LrumTD00JSt4"
   },
   "outputs": [],
   "source": [
    "# load original and motion-corrected movie\n",
    "movie_orig = cm.load(movie_path)\n",
    "movie_corrected = cm.load(mot_correct.mmap_file)\n",
    "# display movie\n",
    "display_movie = True\n",
    "if display_movie:\n",
    "    ds_ratio = 0.2\n",
    "    moviehandle = cm.concatenate([movie_orig.resize(1, 1, ds_ratio) - mot_correct.min_mov*mot_correct.nonneg_movie,\n",
    "                    movie_corrected.resize(1, 1, ds_ratio)], axis=2)\n",
    "    min_, max_ = np.percentile(moviehandle, 0.0001), np.percentile(moviehandle, 99.999)\n",
    "    moviehandle = np.array((moviehandle-min_)/(max_-min_)*255,dtype='uint8')\n",
    "\n",
    "    # create and view mp4 for colap\n",
    "    imageio.mimwrite('/root/caiman_data/motion_correct.mp4', moviehandle, fps = 30, quality=7)\n",
    "    # not sure why this has to be unindented to work\n",
    "    mp4 = open('/root/caiman_data/motion_correct.mp4','rb').read()\n",
    "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "    display(HTML(\"\"\"\n",
    "    <video width=800 controls>\n",
    "          <source src=\"%s\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    \"\"\" % data_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ow1bPTfud9A2"
   },
   "source": [
    "## Load memory mapped file\n",
    "Memory mapping is discussed in some detail in `demo_pipeline.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VajbRy_Od9A2"
   },
   "outputs": [],
   "source": [
    "# load memory mappable file\n",
    "Yr, dims, T = cm.load_memmap(fname_new)\n",
    "images = Yr.T.reshape((T,) + dims, order='F')\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZ3QCMA9Mcra"
   },
   "source": [
    "# Parameter setting for CNMF-E\n",
    "Everything is now set up to run source extraction with CNMFE. We will construct a new parameter dictionary and use this to modify the *existing* `parameters` object, using the `change_params()` method.\n",
    "\n",
    "There are *two* main differences between the CNMF and CNMFE source separation algorithms. The first is the background model (this is discussed in the sidebar below on the Ring Model). The second difference is in how the models are initialized. This is addressed below when we go over setting corr/pnr thresholds for initialization, which we did not have to do for our 2p data.\n",
    "\n",
    "We will explain the important differences in more detail below. For now, note that we have set `gnb` to `0`: this is effectively the flag telling Caiman to use CNMFE instead of CNMF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6UhrwcpMMaaz"
   },
   "outputs": [],
   "source": [
    "# parameters for source extraction and deconvolution\n",
    "p = 1               # order of the autoregressive system\n",
    "K = None            # upper bound on number of components per patch, in general None for CNMFE\n",
    "gSig = np.array([3, 3])  # expected half-width of neurons in pixels\n",
    "gSiz = 2*gSig + 1     # half-width of bounding box created around neurons during initialization\n",
    "merge_thr = .7      # merging threshold, max correlation allowed\n",
    "rf = 40             # half-size of the patches in pixels. e.g., if rf=40, patches are 80x80\n",
    "stride_cnmf = 20    # amount of overlap between the patches in pixels\n",
    "tsub = 2            # downsampling factor in time for initialization, increase if you have memory problems\n",
    "ssub = 1            # downsampling factor in space for initialization, increase if you have memory problems\n",
    "gnb = 0             # number of background components (rank) if positive, set to 0 for CNMFE\n",
    "low_rank_background = None  # None leaves background of each patch intact (use True if gnb>0)\n",
    "nb_patch = 0        # number of background components (rank) per patch (0 for CNMFE)\n",
    "min_corr = .8       # min peak value from correlation image\n",
    "min_pnr = 10        # min peak to noise ration from PNR image\n",
    "ssub_B = 2          # additional downsampling factor in space for background (increase to 2 if slow)\n",
    "ring_size_factor = 1.4  # radius of ring is gSiz*ring_size_factor\n",
    "\n",
    "parameters.change_params(params_dict={'method_init': 'corr_pnr',  # use this for 1 photon\n",
    "                                'K': K,\n",
    "                                'gSig': gSig,\n",
    "                                'gSiz': gSiz,\n",
    "                                'merge_thr': merge_thr,\n",
    "                                'p': p,\n",
    "                                'tsub': tsub,\n",
    "                                'ssub': ssub,\n",
    "                                'rf': rf,\n",
    "                                'stride': stride_cnmf,\n",
    "                                'only_init': True,    # set it to True to run CNMF-E\n",
    "                                'nb': gnb,\n",
    "                                'nb_patch': nb_patch,\n",
    "                                'method_deconvolution': 'oasis',       # could use 'cvxpy' alternatively\n",
    "                                'low_rank_background': low_rank_background,\n",
    "                                'update_background_components': True,  # sometimes setting to False improve the results\n",
    "                                'min_corr': min_corr,\n",
    "                                'min_pnr': min_pnr,\n",
    "                                'normalize_init': False,               # just leave as is\n",
    "                                'center_psf': True,                    # True for 1p\n",
    "                                'ssub_B': ssub_B,\n",
    "                                'ring_size_factor': ring_size_factor,\n",
    "                                'del_duplicates': True,                # whether to remove duplicates from initialization\n",
    "                                'border_pix': bord_px})                # number of pixels to not consider in the borders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDv6X4MgM6-o"
   },
   "source": [
    "Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v6Oln33Nd9A3"
   },
   "outputs": [],
   "source": [
    "cnmfe_model = cnmf.CNMF(n_processes=n_processes,\n",
    "                        dview=cluster,\n",
    "                        params=parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Ahp5GuEM5wI"
   },
   "source": [
    "> <h2>CNMF-E: The background (ring) model</h2>\n",
    ">\n",
    "> <p>Background activity is very ill-behaved with 1p recordings: it often fluctuates locally and is much larger in magnitude than the neural signals we want to extract. In other words, the large-scale background model used for CNMF is not sufficient for most 1p data. Hence, Pengcheng Zhou and others came up with a localized model of background activity for CNMFE: CNMFE represents the background at each pixel as the weighted sum of activity from a circle (or ring) of pixels a certain distance from that pixel. The distance of this ring from the reference pixel is set by the <em>ring_size_factor</em> parameter. This more complex pixel-wise background model explains why CNMFE is computationally more expensive than CNMF, and also why it works better to mop up large-scale localized background noise to find the neurons in your 1p data.</p>\n",
    ">\n",
    "> <p> When you set <em>gnb</em> in the CNMF model (usually to 1 or 2), you are setting the number of global background components to use. The fact that you can get away with so few is testament to how well-behaved the background activity is in 2p recordings compared to 1p. When we set <em>gnb</em> to 0 in Caiman, this is a flag telling Caiman's back end to switch to the ring model of the background activity.</p>\n",
    ">\n",
    "> <p> For more details on CNMFE you can see the <a href=\"https://elifesciences.org/articles/28728\">original paper</a> and the <a href=\"https://elifesciences.org/articles/38173\">Caiman paper</a>.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bsM3WGtLOBXR"
   },
   "source": [
    "## Key parameters for CNMFE\n",
    "The key parameters for CNMFE are slightly different than for CNMF, but with some overlap. As we'll see, because of the high levels of background activity, we can't initialize the same way as with CNMF. We have two new extremely important parameters directly related to initialization that come into play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gFHXmt5OKqh"
   },
   "source": [
    "`rf` (int): *patch half-width*\n",
    "> `rf`, which stands for 'receptive field', is the half width of patches in pixels. The patch width is `2*rf + 1`. `rf` should be *at least* 3-4 times larger than the observed neuron diameter. The larger the patch size, the less parallelization will be used by Caiman. If `rf` is set to `None`, then CNMFE will be run on the entire field of view.\n",
    "\n",
    "`stride_cnmf (int)`: *patch overlap*\n",
    "> `stride_cnmf` is the overlap between patches in pixels (the actual overlap is `stride_cnmf + 1`). This should be at least the diameter of a neuron. The larger the overlap, the greater the computational load, but the results will be more accurate when stitching together results from different patches. This param should probably have been called 'overlap' instead of 'stride'.\n",
    "\n",
    "`gSig (int, int)`: *half-width of neurons*\n",
    "> `gSig` is roughly the half-width of neurons in your movie in pixels (height, width). It is the standard deviation of the mean-centered Gaussian used to filter the movie before initialization for CNMFE. It is related to the `gSiz` parameter, which is the size (in pixels) of a bounding box created around each seed pixel during initilialization. You will usually set `gSiz` to between `2*gSig` and `4*gSig` for CNMFE.\n",
    "\n",
    "`merge_thr (float)`: *merge threshold*\n",
    "> If the correlation between two spatially overlapping components is above `merge_thr`, they will be merged into one component.\n",
    "\n",
    "`min_corr` (float): *minimum correlation*\n",
    "> Pixels from neurons tend to be correlated with their neighbors. For initialization we select for pixels above a minimum correlation `min_corr`.  We discuss this more below.\n",
    "\n",
    "`min_pnr` (float): *minimum peak to noise ratio*\n",
    "> Set a threshoild peak-to-noise ratio. Pixels from neurons tend to have a high signal-to-noise ratio. For initialization we select for pixels above a minimum peak-to-noise-ratio `min_pnr`. We discuss this more below.\n",
    "\n",
    "As we did in `demo_pipeline.ipynb`, let's define a convenience function to get these key params for cnmfe so we can print them as we iteratively muck about in paramter space:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yeNKaYiRONV7"
   },
   "source": [
    "Some functions we will use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fv4Kx01tOV5K"
   },
   "outputs": [],
   "source": [
    "def key_params_cnmfe(cnmfe_model):\n",
    "    \"\"\"\n",
    "    Convenience function to return critical parameters given CNMFE estimator object.\n",
    "    Returns dict with values of rf, stride, gSig, gSiz, merge_threshold, min_corr, min_pnr\n",
    "\n",
    "    Note:\n",
    "    gSiz is included because it depends on gSig and you want to make sure to change it when you change gSig.\n",
    "    These are not set in stone: tweak for your own needs!\n",
    "    \"\"\"\n",
    "    rf = cnmfe_model.params.patch['rf']\n",
    "    stride = cnmfe_model.params.patch['stride']\n",
    "    gSig = cnmfe_model.params.init['gSig']\n",
    "    gSiz = cnmfe_model.params.init['gSiz']\n",
    "    merge_thr = cnmfe_model.params.merging['merge_thr']\n",
    "    min_corr = cnmfe_model.params.init['min_corr']\n",
    "    min_pnr = cnmfe_model.params.init['min_pnr']\n",
    "\n",
    "\n",
    "    key_params = {'min_corr': min_corr,\n",
    "                  'min_pnr': min_pnr,\n",
    "                  'rf': rf,\n",
    "                  'stride': stride,\n",
    "                  'gSig': gSig,\n",
    "                  'gSiz': gSiz,\n",
    "                  'merge_thr': merge_thr}\n",
    "\n",
    "    return key_params\n",
    "\n",
    "def nb_inspect_correlation_pnr(corr, pnr, cmap='jet', num_bins=100):\n",
    "    \"\"\"\n",
    "    inspect correlation and pnr images to infer the min_corr, min_pnr for cnmfe\n",
    "\n",
    "    Args:\n",
    "        corr: ndarray\n",
    "            correlation image created with caiman.summary_images.correlation_pnr\n",
    "\n",
    "        pnr: ndarray\n",
    "            peak-to-noise image created with caiman.summary_images.correlation_pnr\n",
    "\n",
    "        cmap: string\n",
    "            colormap used for plotting corr and pnr images\n",
    "            For valid colormaps see https://holoviews.org/user_guide/Colormaps.html\n",
    "\n",
    "        num_bins: int\n",
    "            number of bins to use for plotting histogram of corr/pnr values\n",
    "\n",
    "    Returns:\n",
    "        Holoviews plot layout (typically just plots in notebook)\n",
    "    \"\"\"\n",
    "    import functools as fct\n",
    "    hv_corr = hv.Image(corr,\n",
    "                       vdims='corr',\n",
    "                       label='correlation').opts(cmap=cmap)\n",
    "    hv_pnr = hv.Image(pnr,\n",
    "                      vdims='pnr',\n",
    "                      label='pnr').opts(cmap=cmap)\n",
    "\n",
    "    def hist(im, rx, ry, num_bins=num_bins):\n",
    "        obj = im.select(x=rx, y=ry) if rx and ry else im\n",
    "        return hv.operation.histogram(obj, num_bins=num_bins)\n",
    "\n",
    "    str_corr = (hv.streams.RangeXY(source=hv_corr).rename(x_range='rx', y_range='ry'))\n",
    "    str_pnr = (hv.streams.RangeXY(source=hv_pnr).rename(x_range='rx', y_range='ry'))\n",
    "\n",
    "    hist_corr = hv.DynamicMap(\n",
    "        fct.partial(hist, im=hv_corr), streams=[str_corr])\n",
    "\n",
    "    hist_pnr = hv.DynamicMap(\n",
    "        fct.partial(hist, im=hv_pnr), streams=[str_pnr])\n",
    "\n",
    "    hv_layout = (hv_corr << hist_corr) + (hv_pnr << hist_pnr)\n",
    "\n",
    "    return hv_layout\n",
    "\n",
    "\n",
    "def get_rectangle_coords(im_dims,\n",
    "                         stride,\n",
    "                         overlap):\n",
    "    \"\"\"\n",
    "    Extract rectangle (patch) coordinates: a helper function used by view_quilt().\n",
    "\n",
    "    Given dimensions of summary image (rows x colums), stride between patches, and overlap\n",
    "    between patches, returns row coordinates of the patches in patch_rows, and column\n",
    "    coordinates patches in patch_cols. This is meant to be used by plot_patches().\n",
    "\n",
    "    Args:\n",
    "        im_dims: array-like\n",
    "            dimension of image (num_rows, num_cols)\n",
    "        stride: int\n",
    "            stride between patches in pixels\n",
    "        overlap: int\n",
    "            overlap between patches in pixels\n",
    "\n",
    "    Returns:\n",
    "        patch_rows: ndarray\n",
    "            num_patch_rows x 2 array, where row i contains onset and offset row pixels for patch row i\n",
    "        patch_cols: ndarray\n",
    "            num_patch_cols x 2 array, where row j contains onset and offset column pixels for patch column j\n",
    "\n",
    "    Note:\n",
    "        Currently assumes square patches so takes in a single number for stride/overlap.\n",
    "    \"\"\"\n",
    "    patch_width = overlap + stride\n",
    "\n",
    "    patch_onset_rows = np.array(list(range(0, im_dims[0] - patch_width, stride)) + [im_dims[0] - patch_width])\n",
    "    patch_offset_rows = patch_onset_rows + patch_width\n",
    "    patch_offset_rows[patch_offset_rows > im_dims[0]-1] = im_dims[0]-1\n",
    "    patch_rows = np.column_stack((patch_onset_rows, patch_offset_rows))\n",
    "\n",
    "    patch_onset_cols = np.array(list(range(0, im_dims[1] - patch_width, stride)) + [im_dims[1] - patch_width])\n",
    "    patch_offset_cols = patch_onset_cols + patch_width\n",
    "    patch_offset_cols[patch_offset_cols > im_dims[1]-1] = im_dims[1]-1\n",
    "    patch_cols = np.column_stack((patch_onset_cols, patch_offset_cols))\n",
    "\n",
    "    return patch_rows, patch_cols\n",
    "\n",
    "\n",
    "def rect_draw(row_minmax,\n",
    "              col_minmax,\n",
    "              color='white',\n",
    "              alpha=0.3,\n",
    "              ax=None):\n",
    "    \"\"\"\n",
    "    Draw a single transluscent rectangle on given axes object.\n",
    "\n",
    "    Args:\n",
    "        row_minmax: array-like\n",
    "            [row_min, row_max] -- 2-elt int bounds for rect rows\n",
    "        col_minmax: array-like\n",
    "            [col_min, col_max] -- int bounds for rect cols\n",
    "        color : string\n",
    "            rectangle color, default 'yellow'\n",
    "        alpha : float\n",
    "            rectangle alpha (0. to 1., where 1 is opaque), default 0.3\n",
    "        ax : pyplot.Axes object\n",
    "            axes object upon which rectangle will be drawn, default None\n",
    "\n",
    "    Returns:\n",
    "        ax (Axes object)\n",
    "        rect (Rectangle object)\n",
    "    \"\"\"\n",
    "    from matplotlib.patches import Rectangle\n",
    "\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    box_origin = (col_minmax[0], row_minmax[0])\n",
    "    box_height = row_minmax[1] - row_minmax[0]\n",
    "    box_width = col_minmax[1] - col_minmax[0]\n",
    "\n",
    "    rect = Rectangle(box_origin,\n",
    "                     width=box_width,\n",
    "                     height=box_height,\n",
    "                     color=color,\n",
    "                     alpha=alpha)\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "    return ax, rect\n",
    "\n",
    "\n",
    "def view_quilt(template_image,\n",
    "               stride,\n",
    "               overlap,\n",
    "               color='white',\n",
    "               alpha=0.2,\n",
    "               vmin=None,\n",
    "               vmax=None,\n",
    "               figsize=(6.,6.),\n",
    "               ax=None):\n",
    "    \"\"\"\n",
    "    Plot patches on template image given stride and overlap parameters on template image.\n",
    "    This can be useful for checking motion correction and cnmf spatial parameters.\n",
    "    It ends up looking like a quilt pattern.\n",
    "\n",
    "    Args:\n",
    "        template_image: ndarray\n",
    "            row x col summary image upon which to draw patches (e.g., correlation image)\n",
    "        stride (int) stride between patches in pixels\n",
    "        overlap (int) overlap between patches in pixels\n",
    "        color: matplotlib color\n",
    "            Any acceptable matplotlib color (r,g,b), string, etc., default 'white'\n",
    "        alpha (float) : patch transparency (0. to 1.: higher is more opaque), default 0.2\n",
    "        vmin (float) : vmin for plotting underlying template image, default None\n",
    "        vmax (float) : vmax for plotting underlying template image, default None\n",
    "        figsize (tuple) : fig size in inches (width, height), default (6.,6.)\n",
    "        ax (pyplot.axes): axes object in case you want to add quilt to existing axes\n",
    "\n",
    "    Returns:\n",
    "        ax: pyplot.Axes object\n",
    "\n",
    "    Example:\n",
    "        # Uses cnm object (cnm) and correlation image (corr_image) as template:\n",
    "        patch_width = 2*cnm.params.patch['rf'] + 1\n",
    "        patch_overlap = cnm.params.patch['stride'] + 1\n",
    "        patch_stride = patch_width - patch_overlap\n",
    "        ax = view_quilt(corr_image, patch_stride, patch_overlap, vmin=0.0, vmax=0.6);\n",
    "    \"\"\"\n",
    "    im_dims = template_image.shape\n",
    "    patch_rows, patch_cols = get_rectangle_coords(im_dims, stride, overlap)\n",
    "\n",
    "    if ax is None:\n",
    "      f, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    ax.imshow(template_image, cmap='gray', vmin=vmin, vmax=vmax)\n",
    "    for patch_row in patch_rows:\n",
    "        for patch_col in patch_cols:\n",
    "            ax, _ = rect_draw(patch_row,\n",
    "                              patch_col,\n",
    "                              color=color,\n",
    "                              alpha=alpha,\n",
    "                              ax=ax)\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWboNJmfOefQ"
   },
   "source": [
    "## Inspect summary images and set parameters\n",
    "### Correlation-pnr plot\n",
    "For CNMFE, Caiman uses the correlation and peak-to-noise (PNR) ratio for initialization, which will both tend to be high in regions that contain neurons. Hence, we set a threshold for both quantitites to remove the low correlation/low pnr regions, and highlight the regions higher in both metrics, those regions most likely to contain neuronal activity.\n",
    "\n",
    "First, we calculate the correlation and pnr maps of the raw motion corrected movie after filtering with a mean-centered Gaussian with standard deviation `gSig` (for more information, see the sidebar below). These calculation can be computationally and memory demanding for large datasets, so we subsample if there are many thousands of frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bk-znGHcOg3H"
   },
   "outputs": [],
   "source": [
    "print(gSig)\n",
    "gsig_tmp = (3,3)\n",
    "correlation_image, peak_to_noise_ratio = cm.summary_images.correlation_pnr(images[::max(T//1000, 1)], # subsample if needed\n",
    "                                                                           gSig=gsig_tmp[0], # used for filter\n",
    "                                                                           swap_dim=False) # change swap dim if output looks weird, it is a problem with tiffile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kfke_Ri0O-Cm"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/EricThomson/image_sandbox/main/images/bokeh_menu.jpg\" align=\"right\" width=\"200\"></img>\n",
    "Using `nb_inspect_correlation_pnr()`, you can inspect the correlation and PNR images to find reasonable threshold values for `min_corr` and `min_pnr`. You can adjust the range of values displayed in the plots shown below by choosing the Y-box select tool (third button from the left -- highlighted in yellow in the accompanying image) and selecting the desired region in the histograms to the right of each image. You can also use the pan button (first button on the left) to zoom/adjust the axis limits in the histogram to make it easier to see the limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3HvdAAyWPUJw"
   },
   "outputs": [],
   "source": [
    "hv.extension('bokeh')\n",
    "nb_inspect_correlation_pnr(correlation_image, peak_to_noise_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fPvFoeTncel1"
   },
   "source": [
    "We are looking for a couple of things in the above plot:\n",
    "\n",
    "1. Did we filter with a `gSig` value small enough so that we aren't blending different neurons together? To see what it is like when this happens, set `gsig_tmp` to `(6,6)` and inspect the above plots.\n",
    "2. More importantly, we want to find the threshold correlation and pnr values so that the *lower* threshold eliminates most of the noise and blood vessels from the plots, leaving behind as many of the neural pixels as possible. For this data it will be at a correlation value lower bound between 0.8 and 0.9, and and pnr lower bound somewhere between 10 and 20 (as with CNMF, there is no perfect value: it is often an iterative search, but keep in mind it is better to have false positives later than false negatives).\n",
    "\n",
    "You can tweak the parameters in the following cell (included are some values that are reasonable):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OmXyQazEce5D"
   },
   "outputs": [],
   "source": [
    "print(key_params_cnmfe(cnmfe_model))\n",
    "\n",
    "gsig_new = gSig # unchanged\n",
    "gsiz_new = gsig_new*2 + 1\n",
    "min_corr_new  = 0.85\n",
    "min_pnr_new = 12\n",
    "\n",
    "cnmfe_model.params.change_params(params_dict={'gSig': gsig_new,\n",
    "                                              'gSiz': gsiz_new,\n",
    "                                              'min_corr': min_corr_new,\n",
    "                                              'min_pnr': min_pnr_new});\n",
    "\n",
    "print(key_params_cnmfe(cnmfe_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJHbFa0XcqRX"
   },
   "source": [
    "> <h2>CNMFE initialization: More on correlation and peak-to-noise-ratio</h2>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/EricThomson/image_sandbox/main/images/mn_centered_gaussian.jpg\" align=\"right\" width=\"200\"></img>\n",
    "\n",
    "> How are correlation and peak-to-noise ratio actually calculated? First Caiman convolves the motion corrected movie with a <i>mean-centered Gaussian</i> (example to the right). The sigma of the Gaussian is <em>gSig</em>, and mean centering is turned on by setting <em>center_psf</em> to <em>True</em>. This mean centering creates a Gaussian with a positive peak in the middle of width <i>approximately</i> <em>gSig/2</em>, surrounded by a negative trench, and sets the outer edge to be zero. This preprocessing filter serves to highlight neuronal peaks and smooth away low-frequency background components.\n",
    ">\n",
    "> The function <em>correlation_pnr()</em> applies this mean-centered Gaussian to each frame of the motion corrected movie and returns the correlation image of that movie, as well as the peak-to-noise-ratio (PNR). The correlation image is the correlation of each pixel with its neighbors. The PNR is the ratio of the maximum magnitude at a pixel to the noise value at that pixel (it is a fast and rough measure of signal-to-noise). As mentioned above, both of these values tend to be higher in actual neurons, and the CNMFE initialization procedure is to set a threshold for both quantities, take their <i>product</i>, and use the peaks in this product map to find <i>seed pixels</i> for initialization of the CNMFE source separation algorithm.\n",
    ">\n",
    "> More details on the initialization procedure used here can be found in the <a href=\"https://elifesciences.org/articles/28728\">CNMFE paper</a>, or just by exploring the code.         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_KQO-VHdUBE"
   },
   "source": [
    "### Quilt plot for spatial parameters\n",
    "As discussed in `demo_pipeline.ipynb`, the other important paramters are those used for dividing the movie into patches for parallelization of the algorithm. The same processe is used for CNMFE. Namely, select `rf` and `stride` parameters so that many 3-4 neuron diameters can fit in each patch, and at least one neuron fits in the overlap region between patches. You can visualize the patches using the `view_quilt()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WnTTOmC8dW8z"
   },
   "outputs": [],
   "source": [
    "# calculate stride and overlap from parameters\n",
    "cnmfe_patch_width = cnmfe_model.params.patch['rf']*2 + 1\n",
    "cnmfe_patch_overlap = cnmfe_model.params.patch['stride'] + 1\n",
    "cnmfe_patch_stride = cnmfe_patch_width - cnmfe_patch_overlap\n",
    "print(f'Patch width: {cnmfe_patch_width} , Stride: {cnmfe_patch_stride}, Overlap: {cnmfe_patch_overlap}');\n",
    "\n",
    "# plot the patches\n",
    "fig, patch_ax = plt.subplots(figsize=(6,6))\n",
    "patch_ax = view_quilt(correlation_image,\n",
    "                      cnmfe_patch_stride,\n",
    "                      cnmfe_patch_overlap,\n",
    "                      vmin=np.percentile(np.ravel(correlation_image),50),\n",
    "                      vmax=np.percentile(np.ravel(correlation_image),99.5),\n",
    "                      color='yellow',\n",
    "                      alpha=0.4,\n",
    "                      ax=patch_ax);\n",
    "patch_ax.set_title(f'CNMFE Patch Width {cnmfe_patch_width}, Overlap {cnmfe_patch_overlap}');\n",
    "mpld3.plugins.connect(fig, mpld3.plugins.Zoom())\n",
    "mpld3.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BYXyXM4id4d8"
   },
   "source": [
    "These patches and overlaps may seem a bit large, but that is ok: our main concern is that they not be too small. If you wanted to change them, you could use `change_params` as described above, and as dicussed in the CNMF notebook.\n",
    "\n",
    "Now that we are happy with our parameters, let's run the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUqeFKgvd8fR"
   },
   "source": [
    "## Run the CNMF-E algorithm\n",
    "This can take a while, given the complexity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IFBpUwJSd_B8"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "cnmfe_model.fit(images);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USEb67Y-d9A8"
   },
   "source": [
    "# Component Evaluation\n",
    "Source extraction typically produces many false positives. Our next step is quality control: separating the results into \"good\" and \"bad\" neurons using two different metrics (discussed in detail in the CNMF notebook):\n",
    "\n",
    "- **Signal-to-noise ratio (SNR)**: a minimum SNR is set for the calcium transients (`min_SNR`).\n",
    "- **Spatial correlation**:  a minimum correlation is set between the shape of each component and the frames in the movie when that component is active (`rval_thr`).\n",
    "\n",
    "> Caiman does *not* use the CNN classifier to sort neurons based on shape for 1p data: the network was trained on 2p data. Hence, we set the `use_cnn` param to `False`.\n",
    "\n",
    "Here we set the two parameters and run `evaluate_components()` to see which pass muster:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L6yPKCwUd9A8"
   },
   "outputs": [],
   "source": [
    "min_SNR = 3            # SNR threshold\n",
    "rval_thr = 0.85    # spatial correlation threshold\n",
    "\n",
    "quality_params = {'min_SNR': min_SNR,\n",
    "                  'rval_thr': rval_thr,\n",
    "                  'use_cnn': False}\n",
    "cnmfe_model.params.change_params(params_dict=quality_params)\n",
    "\n",
    "cnmfe_model.estimates.evaluate_components(images, cnmfe_model.params, dview=cluster)\n",
    "\n",
    "print('*****')\n",
    "print(f\"Total number of components: {len(cnmfe_model.estimates.C)}\")\n",
    "print(f\"Number accepted: {len(cnmfe_model.estimates.idx_components)}\")\n",
    "print(f\"Number rejected: {len(cnmfe_model.estimates.idx_components_bad)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wRXJAPob66k4"
   },
   "source": [
    "# Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tg7Cczur6s-Z"
   },
   "outputs": [],
   "source": [
    "#%% plot contour plots of accepted and rejected components\n",
    "cnmfe_model.estimates.plot_contours_nb(img=correlation_image,\n",
    "                                       idx=cnmfe_model.estimates.idx_components);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vPOLQEG-d9A8"
   },
   "source": [
    "These components look reasonable, if a bit large -- their centers are reasonable but the spatial footprints are quite spread out. If I were persuing this further, it would likely be helpful to re-run CNMFE reducing `gSiz`, which can influence the overall spread of the neurons in space.\n",
    "\n",
    "View traces of accepted and rejected components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9t1XVge1d9A8"
   },
   "outputs": [],
   "source": [
    "cnmfe_model.estimates.nb_view_components(img=correlation_image,\n",
    "                                        idx=cnmfe_model.estimates.idx_components,\n",
    "                                        cmap='viridis', #gray\n",
    "                                        thr=.9); #increase to see full footprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UgpF6pXLd9A9"
   },
   "outputs": [],
   "source": [
    "# rejected components\n",
    "cnmfe_model.estimates.nb_view_components(img=correlation_image,\n",
    "                                        idx=cnmfe_model.estimates.idx_components_bad,\n",
    "                                        cmap='viridis', #gray\n",
    "                                        denoised_color='red',\n",
    "                                        thr=0.9); #increase to see full footprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhXuxTP3d9BB"
   },
   "source": [
    "# A few loose ends\n",
    "We have extracted the calcium traces C, spatial footprints A, and estimated spike counts S, which is the main goal with CNMF. But there are a few important things remaining.\n",
    "\n",
    "##  Deconvolution for 1p?\n",
    "While we haven't discussed deconvolution (the estimation of the spikes that generated the calcium traces in `C`), we suggest treating the spike counts returned for 1p data (in `estimates.S`) with CNMFE with some caution. Currently (as of Fall 2023) we are aware of no no ground-truth data that compares 1p recordings with actual spiking data. There is a *lot* of such data for 2p data, which allows for great comparison of different methods (for instance see [the Spikefinder paper](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006157)).\n",
    "\n",
    "Because of this, most researchers analyze the calcium traces directly for 1p recordings (the data in `estimates.C`) or a normalized version of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWixPnAWd9BB"
   },
   "source": [
    "## Extract $\\Delta F/F$ values\n",
    "Currently in Caiman, we don't return a true dfof value for 1p data because Caiman normalizes to both the baseline fluorescence and background activity, and the background activity in 1p is so ill-behaved (as discussed above in the sidebar on the ring model). This is likely to change soon, but we currently only *detrend* the data but do not normalize to baseline (which explains the warning you will see when you run the following):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1FFcY0q-d9BB"
   },
   "outputs": [],
   "source": [
    "if cnmfe_model.estimates.F_dff is None:\n",
    "    print('Calculating estimates.F_dff')\n",
    "    cnmfe_model.estimates.detrend_df_f(quantileMin=8,\n",
    "                                      frames_window=250,\n",
    "                                      use_residuals=False);  # use denoised data\n",
    "else:\n",
    "    print(\"estimates.F_dff already defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RxFDnu76d9BB"
   },
   "source": [
    "## View some different movie results\n",
    "As with CNMF, the CNMFE model of the original movie is:\n",
    "\n",
    "    original_movie = neural_activity + background + residual\n",
    "    \n",
    "The main between CNMF and CNMFE is the model of the background. We can reconstruct the neural movie as `AC` just as we did in `demo_pipeline.ipynb`. Unfortunately, reconstructing the background activity via the ring model is much more complicated for CNMFE, so we will just punt to a built-in function for that in what follows (`compute_background()`).\n",
    "\n",
    "Get model of neural activity and background activity (note for the neural model we are just including the accepted components):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r1vqUaiPd9BB"
   },
   "outputs": [],
   "source": [
    "neural_activity = cnmfe_model.estimates.A[:, cnmfe_model.estimates.idx_components] @\\\n",
    "                  cnmfe_model.estimates.C[cnmfe_model.estimates.idx_components, :]  # AC\n",
    "neural_movie = cm.movie(neural_activity).reshape(dims + (-1,), order='F').transpose([2, 0, 1])\n",
    "background_model = cnmfe_model.estimates.compute_background(Yr);  # build in function -- explore source code for details\n",
    "bg_movie = cm.movie(background_model).reshape(dims + (-1,), order='F').transpose([2, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Pfph-CakxIE"
   },
   "source": [
    "To view just the movie of pure neural activity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_COmCFyzleoT"
   },
   "outputs": [],
   "source": [
    "ds_ratio = 0.2\n",
    "moviehandle = neural_movie.resize(1, 1, ds_ratio)\n",
    "min_, max_ = np.percentile(moviehandle, 0.0001), np.percentile(moviehandle, 99.9999)\n",
    "moviehandle = np.array((moviehandle-min_)/(max_-min_)*255,dtype='uint8')\n",
    "imageio.mimwrite('/root/caiman_data/demo_movie.mp4', moviehandle, fps = 10,  quality=7)\n",
    "mp4 = open('/root/caiman_data/demo_movie.mp4','rb').read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "display(HTML(\"\"\"\n",
    "    <video width=400 controls>\n",
    "          <source src=\"%s\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    \"\"\" % data_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzlFiqK-k9es"
   },
   "source": [
    "Next we can view just the background model. You will see many regions that are constant such as blood vessels, but also lots of large-scale background flourescence, and some local activity which is is on spatial scales larger than `gSig`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iFZPJ4Gqkz59"
   },
   "outputs": [],
   "source": [
    "ds_ratio = 0.2\n",
    "moviehandle = bg_movie.resize(1, 1, ds_ratio)\n",
    "min_, max_ = np.percentile(moviehandle, 0.0001), np.percentile(moviehandle, 99.9999)\n",
    "moviehandle = np.array((moviehandle-min_)/(max_-min_)*255,dtype='uint8')\n",
    "imageio.mimwrite('/root/caiman_data/demo_movie.mp4', moviehandle, fps = 10,  quality=7)\n",
    "mp4 = open('/root/caiman_data/demo_movie.mp4','rb').read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "display(HTML(\"\"\"\n",
    "    <video width=400 controls>\n",
    "          <source src=\"%s\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    \"\"\" % data_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLEVzAk4H0Au"
   },
   "source": [
    "Note there are a **lot more** things in the main demo notebook at the github repo. For this limited demo we are just giving a flavor of how things will work when you run locally!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "295S3Ulrd9BE"
   },
   "source": [
    "# Clean up open resources\n",
    "We have a few resources we have left open we should take care of.\n",
    "\n",
    "## Shut down cluster\n",
    "To free up processing resources, let's shut down the cluster in case it is still open."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hTPYKZAtd9BE"
   },
   "outputs": [],
   "source": [
    "cm.stop_server(dview=cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KOQlxfCUd9BE"
   },
   "source": [
    "## Shut down logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M0EwOcF_d9BE"
   },
   "outputs": [],
   "source": [
    "# Shut off logger\n",
    "logging.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HdAO7OctGVZc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
