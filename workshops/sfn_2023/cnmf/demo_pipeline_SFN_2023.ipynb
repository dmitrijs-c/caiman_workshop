{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is a Colab-friendly update to CaImAn's demo notebook (demo_pipeline.ipynb), and has been adapted for use at the 2023 SFN Satellite Workshop.\n",
        "\n",
        "Thanks to Oliver Barnstedt, Felix Kuhn, Arjun Putcha for contributions to this notebook.\n",
        "\n",
        "> You can access data using `from google.colab import drive` and mount the drive with commands like `drive.mount('/content/drive')` (which will require google login). You will see your files become populated when we download Caiman demos etc. You can remove directories in Colab using `shutil.rmtree('dirname')` but be very careful this is nonreversible."
      ],
      "metadata": {
        "id": "8BJoVp81_sdz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXkvua__d9Aq"
      },
      "source": [
        "# CNMF demo pipeline: Intro\n",
        "This demo presents a full pipeline for the analysis of a two-photon calcium imaging dataset using the CaImAn (**Ca**lcium **Im**aging **An**alysis) software package. Starting with loading the original movie, it demonstrates how to use Caiman's built-in tools for the following analysis steps:    \n",
        "    \n",
        "    \n",
        "![cnmf pipeline full](https://raw.githubusercontent.com/EricThomson/image_sandbox/main/images/full_cnmf_workflow.jpg)\n",
        "    \n",
        "\n",
        "1) Apply the NoRMCorre (nonrigid motion correction) algorithm for motion correction.\n",
        "2) Apply the constrained nonnegative matrix factorization (CNMF) source separation algorithm to extract an initial estimate of neuronal spatial footprint and calcium traces.  \n",
        "3) Apply quality control metrics to evaluate the initial estimates, and narrow down to the final set of estimates.\n",
        "\n",
        "The CNMF algorithm is best for data with relatively low background noise, like most two-photon data and *some* one photon data (e.g., certain light sheet data). For a demo analysis pipeline of a one-photon microendoscopic data set see `demo_pipeline_cnmfE.ipynb`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bh2kN0S_6So5"
      },
      "source": [
        "## Installation\n",
        "This can take a few minutes we are installing Caiman from scratch.\n",
        "\n",
        "You will likely get some `XDG_RUNDIME_DIR` errors this is fine. Everything will still work."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone caiman from github\n",
        "!git clone https://github.com/flatironinstitute/CaImAn.git"
      ],
      "metadata": {
        "id": "sr-azHZIOYQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install relevant packages\n",
        "%cd /content/CaImAn\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "kjhIkUwYTP18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTSnvJOf6Uo8"
      },
      "outputs": [],
      "source": [
        "# Install CaImAn (dev install)\n",
        "!pip install -e .\n",
        "!pip install mpld3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use caimanmanager to install caiman utilities in root directory\n",
        "!caimanmanager install --inplace  # may get error: XDG_RUNTIME_DIR not set this is fine"
      ],
      "metadata": {
        "id": "cIxlimCATZYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbAEyPLbd9At"
      },
      "source": [
        "### Imports and general setup\n",
        "We first need to import the Python libraries we will use in the rest of the notebook and tweak some general settings. Don't worry about these details now, we will explain the important things when they come up. Note in the following, we import `caiman` as `cm`, so when you see `cm` in the rest of the notebook, it just means you are using something from the Caiman package.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyWbhQUtd9At"
      },
      "outputs": [],
      "source": [
        "import bokeh.plotting as bpl\n",
        "import cv2\n",
        "import datetime\n",
        "import glob\n",
        "import holoviews as hv\n",
        "import imageio\n",
        "from IPython import get_ipython\n",
        "import logging\n",
        "import matplotlib.pyplot as plt\n",
        "import mpld3\n",
        "import numpy as np\n",
        "import os\n",
        "import psutil\n",
        "from pathlib import Path\n",
        "\n",
        "try:\n",
        "    cv2.setNumThreads(0)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    if __IPYTHON__:\n",
        "        get_ipython().run_line_magic('load_ext', 'autoreload')\n",
        "        get_ipython().run_line_magic('autoreload', '2')\n",
        "except NameError:\n",
        "    pass\n",
        "\n",
        "import caiman as cm\n",
        "from caiman.motion_correction import MotionCorrect\n",
        "from caiman.source_extraction.cnmf import cnmf as cnmf\n",
        "from caiman.source_extraction.cnmf import params as params\n",
        "from caiman.utils.utils import download_demo\n",
        "from caiman.utils.visualization import plot_contours, nb_view_patches, nb_plot_contour\n",
        "from caiman.summary_images import local_correlations_movie_offline\n",
        "import psutil\n",
        "from scipy.ndimage import center_of_mass\n",
        "from IPython.display import display, clear_output\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "import bokeh\n",
        "import bokeh.plotting as bpl\n",
        "from bokeh.io import output_notebook\n",
        "import holoviews as hv\n",
        "output_notebook()\n",
        "hv.extension('bokeh')\n",
        "%env HV_DOC_HTML=true"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go17YYgHd9At"
      },
      "source": [
        "Continuing with our basic setup, we will set up a logger, and also set some environment variables in case that wasn't done already in your shell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBW0IWVPd9At"
      },
      "outputs": [],
      "source": [
        "# set up logging\n",
        "logging.basicConfig(format=\"{asctime} - {levelname} - [{filename} {funcName}() {lineno}] - pid {process} - {message}\",\n",
        "                    filename=None,\n",
        "                    level=logging.WARNING, style=\"{\") #logging level can be DEBUG, INFO, WARNING, ERROR, CRITICAL\n",
        "\n",
        "# set env variables\n",
        "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
        "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
        "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJ01yrm0d9Au"
      },
      "source": [
        "## Specify data to be processed\n",
        "For this demo, we will analyze the data file `Sue_2x_3000_40_-46.tif`. This 3000-frame movie, provided courtesy of Sue Koay and David Tank (Princeton University), is two-photon data from supragranular parietal cortex of a GCaMP6f-expressing mouse during a virtual reality task. It was collected at 30Hz, and to save space, the demo data has been spatially cropped and downsampled by a factor of 2 compared to the original.  \n",
        "\n",
        "To get the data, we will use Caiman's built-in `download_demo()` function. It will download the data to  `~/caiman_data/example_movies/` where `~` is your home directory (the path format and home directory will depend on your operating system). If you already have the movie, it will just return the path to the movie."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bksDwMKld9Av"
      },
      "outputs": [],
      "source": [
        "movie_path = download_demo('Sue_2x_3000_40_-46.tif')\n",
        "print(f\"Original movie for demo is in {movie_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP5NBTLAd9Av"
      },
      "source": [
        "## Load and visualize raw data\n",
        "Caiman has a powerful built-in movie class that you can use to load,view, process, and save your movie. Unfortunately, it doesn't work in colab.\n",
        "\n",
        "The following workaround is useful for viewing in colab. ðŸ˜€"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lVIf7_V8jTW"
      },
      "outputs": [],
      "source": [
        "# in colab  -- 40 sec runtime\n",
        "# load original movie\n",
        "# display movie\n",
        "movie_orig = cm.load(movie_path)\n",
        "display_movie = True\n",
        "if display_movie:\n",
        "    ds_ratio = 0.2\n",
        "    moviehandle = movie_orig.resize(1, 1, ds_ratio)\n",
        "    min_, max_ = np.percentile(moviehandle, 0.01), np.percentile(moviehandle, 99.99)\n",
        "    moviehandle = np.array((moviehandle-min_)/(max_-min_)*255,dtype='uint8')\n",
        "    imageio.mimwrite('/root/caiman_data/demo_movie.mp4', moviehandle, fps = 10,  quality=7)\n",
        "    mp4 = open('/root/caiman_data/demo_movie.mp4','rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "    display(HTML(\"\"\"\n",
        "    <video width=400 controls>\n",
        "          <source src=\"%s\" type=\"video/mp4\">\n",
        "    </video>\n",
        "    \"\"\" % data_url))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmQ15isLd9Ax"
      },
      "source": [
        "Let's also create a couple of summary images of the movie, including a *maximum projection* (the maximum value of each pixel) and a *correlation image* (how correlated each pixel is with its neighbors). If a pixel comes from an active neural component it will tend to be highly correlated with its neighbors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_bw6FeFd9Ax"
      },
      "outputs": [],
      "source": [
        "max_projection_orig = np.max(movie_orig, axis=0)\n",
        "correlation_image_orig = cm.summary_images.local_correlations_fft(movie_orig, swap_dim=False)\n",
        "correlation_image_orig[np.isnan(correlation_image_orig)] = 0 # get rid of NaNs, if they exist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJkngnJVd9Ax"
      },
      "outputs": [],
      "source": [
        "f, (ax_max, ax_corr) = plt.subplots(1,2,figsize=(6,3))\n",
        "ax_max.imshow(max_projection_orig,\n",
        "              cmap='viridis',\n",
        "              vmin=np.percentile(np.ravel(max_projection_orig),50),\n",
        "              vmax=np.percentile(np.ravel(max_projection_orig),99.5));\n",
        "ax_max.set_title(\"Max Projection Orig\", fontsize=12);\n",
        "\n",
        "ax_corr.imshow(correlation_image_orig,\n",
        "               cmap='viridis',\n",
        "               vmin=np.percentile(np.ravel(correlation_image_orig),50),\n",
        "               vmax=np.percentile(np.ravel(correlation_image_orig),99.5));\n",
        "ax_corr.set_title('Correlation Image Orig', fontsize=12);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwvlXbMId9Ay"
      },
      "source": [
        "These images will not be particularly sharp yet, as there is still a good deal of motion in the movie."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfG787_Pd9Ay"
      },
      "source": [
        "## Set initial parameters\n",
        "In general in Caiman, estimators are first initialized with a set of parameters, and then they are fit against actual data in a separate step. In this section, we'll define a `parameters` object that will subsequently be used to initialize our different estimators.\n",
        "\n",
        "The parameters are divided into different categories. We will not discuss them in detail in this section, but will go over them when needed (and note that in this notebook we will mostly focus on CNMF and later steps):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNMUg02_d9Ay"
      },
      "outputs": [],
      "source": [
        "# general dataset-dependent parameters\n",
        "fr = 30                     # imaging rate in frames per second\n",
        "decay_time = 0.4            # length of a typical transient in seconds\n",
        "dxy = (2., 2.)              # spatial resolution in x and y in (um per pixel)\n",
        "\n",
        "# motion correction parameters\n",
        "strides = (48, 48)          # start a new patch for pw-rigid motion correction every x pixels\n",
        "overlaps = (24, 24)         # overlap between patches (width of patch = strides+overlaps)\n",
        "max_shifts = (6,6)          # maximum allowed rigid shifts (in pixels)\n",
        "max_deviation_rigid = 3     # maximum shifts deviation allowed for patch with respect to rigid shifts\n",
        "pw_rigid = True             # flag for performing non-rigid motion correction\n",
        "\n",
        "# CNMF parameters for source extraction and deconvolution\n",
        "p = 1                       # order of the autoregressive system (set p=2 if there is visible rise time in data)\n",
        "gnb = 2                     # number of global background components (set to 1 or 2)\n",
        "merge_thr = 0.85            # merging threshold, max correlation allowed\n",
        "bas_nonneg = False          # enforce nonnegativity constraint on calcium traces (technically on baseline)\n",
        "rf = 15                     # half-size of the patches in pixels (patch width is rf*2 + 1)\n",
        "stride_cnmf = 6             # amount of overlap between the patches in pixels (overlap is stride_cnmf+1)\n",
        "K = 4                       # number of components per patch\n",
        "gSig = np.array([4, 4])     # expected half-width of neurons in pixels\n",
        "gSiz = 2*gSig + 1           # half-width of bounding box created around neurons during initialization\n",
        "method_init = 'greedy_roi'  # initialization method (if analyzing dendritic data see demo_dendritic.ipynb)\n",
        "ssub = 1                    # spatial subsampling during initialization\n",
        "tsub = 1                    # temporal subsampling during intialization\n",
        "\n",
        "# parameters for component evaluation\n",
        "min_SNR = 2.0               # signal to noise ratio for accepting a component\n",
        "rval_thr = 0.85             # space correlation threshold for accepting a component\n",
        "cnn_thr = 0.99              # threshold for CNN based classifier\n",
        "cnn_lowest = 0.1            # neurons with cnn probability lower than this value are rejected"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vriA7c-Pd9Ay"
      },
      "source": [
        "We place the above parameter values in a dictionary that is passed to the `CNMFParams` class (those parameters *not* explicitly defined will assume default values):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtZA2SRmd9Ay"
      },
      "outputs": [],
      "source": [
        "parameter_dict = {'fnames': movie_path,\n",
        "                  'fr': fr,\n",
        "                  'dxy': dxy,\n",
        "                  'decay_time': decay_time,\n",
        "                  'strides': strides,\n",
        "                  'overlaps': overlaps,\n",
        "                  'max_shifts': max_shifts,\n",
        "                  'max_deviation_rigid': max_deviation_rigid,\n",
        "                  'pw_rigid': pw_rigid,\n",
        "                  'p': p,\n",
        "                  'nb': gnb,\n",
        "                  'rf': rf,\n",
        "                  'K': K,\n",
        "                  'gSig': gSig,\n",
        "                  'gSiz': gSiz,\n",
        "                  'stride': stride_cnmf,\n",
        "                  'method_init': method_init,\n",
        "                  'rolling_sum': True,\n",
        "                  'only_init': True,\n",
        "                  'ssub': ssub,\n",
        "                  'tsub': tsub,\n",
        "                  'merge_thr': merge_thr,\n",
        "                  'bas_nonneg': bas_nonneg,\n",
        "                  'min_SNR': min_SNR,\n",
        "                  'rval_thr': rval_thr,\n",
        "                  'use_cnn': True,\n",
        "                  'min_cnn_thr': cnn_thr,\n",
        "                  'cnn_lowest': cnn_lowest}\n",
        "\n",
        "parameters = params.CNMFParams(params_dict=parameter_dict) # CNMFParams is the parameters class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4taKOxmd9Az"
      },
      "source": [
        "This parameters object (`parameters`) is basically a collection of dictionaries, each containing a different parameter category. These different dictionaries can be accessed using dot notation.  Some parameters are related to the dataset in general (`parameters.data`), while most are related to specific aspects of the workflow such as motion correction (`parameters.motion`) or quality evaluation (`parameters.quality`).\n",
        "\n",
        "For instance, if you want to inspect the dataset-dependent parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SFoZqIZd9Az"
      },
      "outputs": [],
      "source": [
        "parameters.data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbl8J_11d9Az"
      },
      "source": [
        "To access a particular parameter in this parameter field is just to access a value of a dictionary using the appropriate key. So to get the frame rate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnLydiNJd9Az"
      },
      "outputs": [],
      "source": [
        "parameters.data['fr']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqGVVgbhd9Az"
      },
      "source": [
        "## Setting up a cluster\n",
        "Caiman is optimized for parallel computing, and distributes computations to multiple CPU cores for motion correction and CNMF (there is also an option to run motion correction on the GPU, but we will not focus on that here). Setting up the multicore processing is done with the `setup_cluster()` function below.\n",
        "\n",
        "First, let's see how many CPUs we have available, and set the number of processors we want to use. If you set `num_processors_to_use` to `None`, then `setup_cluster()` will set it one less than the total number available.\n",
        "\n",
        "Colab provides two CPUs by default for the free service, so let's use all of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNYM6w0ad9A0"
      },
      "outputs": [],
      "source": [
        "print(f\"You have {psutil.cpu_count()} CPUs available in your current environment\")\n",
        "num_processors_to_use = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3sCUhq0d9A0"
      },
      "source": [
        "Set up a cluster of processors. If one has already been set up (the `cluster` variable is already in your namespace), then that cluster will be closed and a new one created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXvozQVdd9A0"
      },
      "outputs": [],
      "source": [
        "if 'cluster' in locals():  # 'locals' contains list of current local variables\n",
        "    print('Closing previous cluster')\n",
        "    cm.stop_server(dview=cluster)\n",
        "print(\"Setting up new cluster\")\n",
        "_, cluster, n_processes = cm.cluster.setup_cluster(backend='multiprocessing',\n",
        "                                                   n_processes=num_processors_to_use,\n",
        "                                                   ignore_preexisting=False)\n",
        "print(f\"Successfully set up cluster with {n_processes} processes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhQD3Y4cd9A0"
      },
      "source": [
        "The output `cluster` is the pool of processors (CPUs) that will be used in many of Caiman's subsequent processing steps. In these later steps, if you set the parameter `dview` to `cluster`, then parallel processing will be used. If instead you set `dview` to `None` then no parallel processing will be used. This latter option can be important when debugging, as the logger doesn't typically work for multithreaded operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3g7trTxd9A0"
      },
      "source": [
        "# Motion Correction\n",
        "The first substantive step in our analysis pipeline is to remove motion artifacts from the original movie:\n",
        "\n",
        "![Txt](https://raw.githubusercontent.com/EricThomson/image_sandbox/main/images/normcorre_workflow.jpg)\n",
        "\n",
        "It is *very* important to get rid of motion artifacts, as the subsequent CNMF source separation algorithm assumes that each pixel represents the same region of space\n",
        "\n",
        "First, we initialize the motion correction estimator using the parameters that we set above:   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAmpSbMvd9A1"
      },
      "outputs": [],
      "source": [
        "mot_correct = MotionCorrect(movie_path,\n",
        "                            dview=cluster,\n",
        "                            **parameters.motion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSD3tGmxd9A1"
      },
      "source": [
        "This notebook focuses on CNMF, not motion correction, but let's consider a couple of the motion correction parameters that were set above:\n",
        "\n",
        "- `pw_rigid=True` tells us that we are going to perform piecewise rigid motion correction using the nonrigid motion correction (NoRMCorre) algorithm (this is because the data seems to exhibit some non-uniform motion). If your data exhibits uniform motion across the field of view, set this to `False` for efficiency.\n",
        "- NoRMCorre will split the movie into patches that repeat every 48 pixels (`strides`), and have 24 pixels of overlap (`overlaps`): the total patch width is 72 (the sum of stride and overlap)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8enKVk7d9A1"
      },
      "source": [
        "The next step is to actually perform motion correction using the `motion_correct()` method on the estimator we built. You may see some warnings about negative movie averages: you can ignore them.\n",
        "\n",
        "Note if you are running in colab, this can take some time (about four minutes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7BA9YkXd9A1"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "#%% Run piecewise-rigid motion correction using NoRMCorre -- runtime 4min\n",
        "mot_correct.motion_correct(save_movie=True);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtQFqqxcd9A1"
      },
      "source": [
        "Inspect the results by comparing the original movie\n",
        "\n",
        "As discussed above, if we were running Caiman locally, we would make heavier use of Caiman's built-in movie object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrumTD00JSt4"
      },
      "outputs": [],
      "source": [
        "# load original and motion-corrected movie\n",
        "movie_orig = cm.load(movie_path)\n",
        "movie_corrected = cm.load(mot_correct.mmap_file)\n",
        "# display movie\n",
        "display_movie = True\n",
        "if display_movie:\n",
        "    ds_ratio = 0.2\n",
        "    moviehandle = cm.concatenate([movie_orig.resize(1, 1, ds_ratio) - mot_correct.min_mov*mot_correct.nonneg_movie,\n",
        "                    movie_corrected.resize(1, 1, ds_ratio)], axis=2)\n",
        "    min_, max_ = np.percentile(moviehandle, 0.001), np.percentile(moviehandle, 99.999)\n",
        "    moviehandle = np.array((moviehandle-min_)/(max_-min_)*255,dtype='uint8')\n",
        "\n",
        "    # create and view mp4 for colap\n",
        "    imageio.mimwrite('/root/caiman_data/motion_correct.mp4', moviehandle, fps = 30, quality=7)\n",
        "    # not sure why this has to be unindented to work\n",
        "    mp4 = open('/root/caiman_data/motion_correct.mp4','rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "    display(HTML(\"\"\"\n",
        "    <video width=800 controls>\n",
        "          <source src=\"%s\" type=\"video/mp4\">\n",
        "    </video>\n",
        "    \"\"\" % data_url))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is also possible to view the 'shifts' made to stabilize the movie. Because we used piecewise rigid motion correction, there are both 'rigid' and 'nonrigid' or 'piecewise' shifts we can view from the different patches."
      ],
      "metadata": {
        "id": "2dCP44ZfWzWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot rigid and nonrigid shifts\n",
        "plt.figure(figsize = (10,8))\n",
        "plt.subplot(3,1,1)\n",
        "plt.plot(mot_correct.shifts_rig)\n",
        "plt.title('Rigid Shifts')\n",
        "plt.legend(['x shifts', 'y shifts'])\n",
        "plt.xlabel('frames')\n",
        "plt.ylabel('pixels')\n",
        "\n",
        "plt.subplot(3,1,2)\n",
        "plt.plot(mot_correct.x_shifts_els)\n",
        "plt.title('Patch X Shifts')\n",
        "plt.ylabel('x shifts (pixels)')\n",
        "\n",
        "plt.subplot(3,1,3)\n",
        "plt.plot(mot_correct.y_shifts_els)\n",
        "plt.title('Patch Y Shifts')\n",
        "plt.ylabel('y shifts (pixels)')\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "KG9kuchwW2Gu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhRrHmeXd9A2"
      },
      "source": [
        "Let's look at the max projection and correlation image of the motion corrected movies. In movies that originally contained a lot of movement, the summary images will look more \"crisp\" than in the originals because they are no longer blurred by movement:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_projection = np.max(movie_corrected, axis=0)\n",
        "correlation_image = cm.summary_images.local_correlations_fft(movie_corrected, swap_dim=False)\n",
        "correlation_image[np.isnan(correlation_image)] = 0 # get rid of NaNs, if they exist"
      ],
      "metadata": {
        "id": "KiI0bQFS22If"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJxIcFXLd9A2"
      },
      "outputs": [],
      "source": [
        "f, ((ax_max_orig, ax_max), (ax_corr_orig, ax_corr)) = plt.subplots(2,2,figsize=(6,6), sharex=True, sharey=True)\n",
        "# plot max projection\n",
        "ax_max_orig.imshow(max_projection_orig,\n",
        "                   cmap='viridis',\n",
        "                   vmin=np.percentile(np.ravel(max_projection_orig),50),\n",
        "                   vmax=np.percentile(np.ravel(max_projection_orig),99.5));\n",
        "ax_max_orig.set_title(\"Max Projection Orig\", fontsize=12);\n",
        "ax_max.imshow(max_projection,\n",
        "              cmap='viridis',\n",
        "              vmin=np.percentile(np.ravel(max_projection),50),\n",
        "              vmax=np.percentile(np.ravel(max_projection),99.5));\n",
        "ax_max.set_title(\"Max Motion Corrected\", fontsize=12);\n",
        "\n",
        "# plot correlation image\n",
        "ax_corr_orig.imshow(correlation_image_orig,\n",
        "                    cmap='viridis',\n",
        "                   vmin=np.percentile(np.ravel(correlation_image_orig),50),\n",
        "                   vmax=np.percentile(np.ravel(correlation_image_orig),99.5));\n",
        "ax_corr_orig.set_title('Correlation Image Orig', fontsize=12);\n",
        "ax_corr.imshow(correlation_image,\n",
        "               cmap='viridis',\n",
        "               vmin=np.percentile(np.ravel(correlation_image),50),\n",
        "               vmax=np.percentile(np.ravel(correlation_image),99.5));\n",
        "ax_corr.set_title('Correlation Motion Corrected', fontsize=12);\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ow1bPTfud9A2"
      },
      "source": [
        "## Creating and accessing memory mapped files\n",
        "The motion-corrected data was saved as as a memory mapped file in `mot_correct.mmap_file` (in F-order, which was optimized for motion correction). The cell below saves the same motion corrected data in a second memory mapped file in C-order, which is optimized for CNMF. We then load the memmapped data with the built-in caiman function `load_memmap()`. This lets us treat the memory-mapped data *as if* it were in memory while leaving it on disk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VajbRy_Od9A2"
      },
      "outputs": [],
      "source": [
        "border_to_0 = 0 if mot_correct.border_nan == 'copy' else mot_correct.border_to_0 # trim border against NaNs\n",
        "mc_memmapped_fname = cm.save_memmap(mot_correct.mmap_file,\n",
        "                                        base_name='memmap_',\n",
        "                                        order='C',\n",
        "                                        border_to_0=border_to_0,  # exclude borders, if that was done\n",
        "                                        dview=cluster)\n",
        "Yr, dims, num_frames = cm.load_memmap(mc_memmapped_fname)\n",
        "images = np.reshape(Yr.T, [num_frames] + list(dims), order='F') #reshape frames in standard 3d format (T x X x Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7uZW16od9A3"
      },
      "source": [
        "Here, we have used a caiman built-in function to load the memory mapped file to variable `Yr`, which is is the motion corrected movie reshaped so that each frame is in a single column. We have also created a version `images` in more standard movie format (`num_frames x cols x rows`). All of these arrays are memory mapped, so are not loaded into RAM, but reference data on disk.\n",
        "\n",
        "Restart the cluster to clean up memory in prep for CNMF run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dh_4s5Uid9A3"
      },
      "outputs": [],
      "source": [
        "cm.stop_server(dview=cluster)\n",
        "_, cluster, n_processes = cm.cluster.setup_cluster(backend='multiprocessing',\n",
        "                                                   n_processes=num_processors_to_use,\n",
        "                                                   single_thread=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# colab memory clearing\n",
        "del moviehandle, movie_corrected, movie_orig"
      ],
      "metadata": {
        "id": "-PLjUflVN9AO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6Oln33Nd9A3"
      },
      "source": [
        "Some functions we will use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wet__Rfad9A4"
      },
      "outputs": [],
      "source": [
        "def key_params(cnmf_model):\n",
        "    \"\"\"\n",
        "    Convenience function to return critical parameters given CNMF estimator object.\n",
        "    Returns dictionary with values of rf, stride, gSig, gSiz, K, merge_threshold\n",
        "\n",
        "    Note:\n",
        "    gSiz is included because it depends on gSig and you want to make sure to change it when you change gSig.\n",
        "    These are not set in stone: tweak for your own needs!\n",
        "    \"\"\"\n",
        "    rf = cnmf_model.params.patch['rf']\n",
        "    stride = cnmf_model.params.patch['stride']\n",
        "    gSig = cnmf_model.params.init['gSig']\n",
        "    gSiz = cnmf_model.params.init['gSiz']\n",
        "    K = cnmf_model.params.init['K']\n",
        "    merge_thr = cnmf_model.params.merging['merge_thr']\n",
        "\n",
        "    key_params = {'rf': rf,\n",
        "                  'stride': stride,\n",
        "                  'gSig': gSig,\n",
        "                  'gSiz': gSiz,\n",
        "                  'K': K,\n",
        "                  'merge_thr': merge_thr}\n",
        "\n",
        "    return key_params\n",
        "\n",
        "\n",
        "\n",
        "def view_quilt(template_image,\n",
        "               stride,\n",
        "               overlap,\n",
        "               color='white',\n",
        "               alpha=0.2,\n",
        "               vmin=None,\n",
        "               vmax=None,\n",
        "               figsize=(6.,6.),\n",
        "               ax=None):\n",
        "    \"\"\"\n",
        "    Plot patches on template image given stride and overlap parameters on template image.\n",
        "    This can be useful for checking motion correction and cnmf spatial parameters.\n",
        "    It ends up looking like a quilt pattern.\n",
        "\n",
        "    Args:\n",
        "        template_image: ndarray\n",
        "            row x col summary image upon which to draw patches (e.g., correlation image)\n",
        "        stride (int) stride between patches in pixels\n",
        "        overlap (int) overlap between patches in pixels\n",
        "        color: matplotlib color\n",
        "            Any acceptable matplotlib color (r,g,b), string, etc., default 'white'\n",
        "        alpha (float) : patch transparency (0. to 1.: higher is more opaque), default 0.2\n",
        "        vmin (float) : vmin for plotting underlying template image, default None\n",
        "        vmax (float) : vmax for plotting underlying template image, default None\n",
        "        figsize (tuple) : fig size in inches (width, height), default (6.,6.)\n",
        "        ax (pyplot.axes): axes object in case you want to add quilt to existing axes\n",
        "\n",
        "    Returns:\n",
        "        ax: pyplot.Axes object\n",
        "\n",
        "    Example:\n",
        "        # Uses cnm object (cnm) and correlation image (corr_image) as template:\n",
        "        patch_width = 2*cnm.params.patch['rf'] + 1\n",
        "        patch_overlap = cnm.params.patch['stride'] + 1\n",
        "        patch_stride = patch_width - patch_overlap\n",
        "        ax = view_quilt(corr_image, patch_stride, patch_overlap, vmin=0.0, vmax=0.6);\n",
        "    \"\"\"\n",
        "    from caiman.utils.visualization import rect_draw, get_rectangle_coords\n",
        "\n",
        "    im_dims = template_image.shape\n",
        "    patch_rows, patch_cols = get_rectangle_coords(im_dims, stride, overlap)\n",
        "\n",
        "    if ax is None:\n",
        "      f, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    ax.imshow(template_image, cmap='gray', vmin=vmin, vmax=vmax)\n",
        "    for patch_row in patch_rows:\n",
        "        for patch_col in patch_cols:\n",
        "            ax, _ = rect_draw(patch_row,\n",
        "                              patch_col,\n",
        "                              color=color,\n",
        "                              alpha=alpha,\n",
        "                              ax=ax)\n",
        "\n",
        "    return ax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTuJxuJ5d9A4"
      },
      "source": [
        "# Run CNMF on patches in parallel\n",
        "\n",
        "Everything is now set up for running CNMF. This algorithm simultaneously extracts the *spatial footprint* and corresponding *calcium trace* for each component.\n",
        "\n",
        "![Txt](https://raw.githubusercontent.com/EricThomson/image_sandbox/main/images/cnmf_workflow.jpg)\n",
        "\n",
        "It also performs *deconvolution*, providing an estimate of the spike count that generated the calcium signal in the movie."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrpEKRkdd9A4"
      },
      "source": [
        "The algorithm is parallelized as illustrated here:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/EricThomson/image_sandbox/main/images/cnmf_patches.jpg\" alt=\"cnmf patch flow\" width=\"500\"/>\n",
        "\n",
        "1. The movie field of view is split into overlapping patches.\n",
        "2. These patches are processed in parallel by the CNMF algorithm. The degree of parallelization depends on your available computing power: if you have just one CPU then the chunks will be processed sequentially.\n",
        "3. The results from all the patches are merged, with special focus on components in overlapping regions -- overlapping components are merged if their activity is highly correlated.\n",
        "4. Results are refined with additional iterations of CNMF.\n",
        "\n",
        "As discussed above, Caiman's main algorithms are run in two steps: first the estimators are *initialized* with a set of parameters, and then they are *fit* against actual data. Let's initialize our CNMF estimator object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WJlG2Rod9A4"
      },
      "outputs": [],
      "source": [
        "cnmf_model = cnmf.CNMF(n_processes,\n",
        "                       params=parameters,\n",
        "                       dview=cluster)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNnZSG1dd9A5"
      },
      "source": [
        "Once initialized, we we could immediately run `cnmf_model.fit(images)` if we knew the parameters were good. The parameters in your CNMF estimator object are accessable in `cnmf_model.params`. However, before running the algorithm, let's discuss the more critical parameters that you will most likely need to change in the future when running CNMF on your own data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7bSGGgWd9A5"
      },
      "source": [
        "### Key parameters for CNMF\n",
        "\n",
        "`rf (int)`: *patch half-width*\n",
        "\n",
        "> `rf`, which stands for 'receptive field', is the half width of patches in pixels. The full patch width is `2*rf + 1`. `rf` should be *at least* 3-4 times larger than the expected neuron diameter. This is so that the CNMF algorithm will be able to properly estimate local background activity. Also the larger the patch size, the less parallelization will be used by Caiman. If `rf` is set to `None`, then no parallelization will be used and CNMF will be run on the entire field of view.\n",
        "\n",
        "`stride_cnmf (int)`: *patch overlap*\n",
        "\n",
        "> `stride_cnmf` is the overlap between patches in pixels (the actual overlap is `stride_cnmf + 1`). This should be at least the diameter of a neuron. The larger the overlap, the greater the computational load, but the results will be more accurate when stitching together results from different patches. This param should probably be called 'overlap' instead of 'stride'.\n",
        "\n",
        "\n",
        "`gSig (int, int)`: *half-width of neurons*\n",
        "\n",
        "> `gSig` is roughly the half-width of neurons in your movie in pixels (height, width). Used during initialization, technically it is the sigma parameter in a Gaussian filter run on all the images. If the filter is appropriately matched to your data, you will get a much better estimate, so this is one of the most important parameters. It also determines the value of `gSiz`, which is set to `2*gSig + 1` for CNMF. `gSiz` is the width of a bounding box created around each seed pixel during initilialization -- if after running `refit()` below your neurons end up looking square or artificially cut off, you may need to increase `gSiz`.\n",
        "    \n",
        "    \n",
        "`K (int)`: *components per patch*\n",
        "\n",
        "> `K` is the number of components you expect per patch. You should obviously adapt this to the density of components in your data, and the current `rf` parameter. We suggest pick `K` based on the more dense patches in your movie, as it is easy to remove false positives later in the component evaluation stage. However, if you pick `K` much too high, the algorithm will take longer to run, and the false positives will become inconvenient to sift through later.\n",
        "    \n",
        "`merge_thr (float)`: *merge threshold*\n",
        "\n",
        "> `merge_thr` is the merge threshold. If two spatially overlapping components are correlated above this value, they will be merged into one component. The correlation coefficient is calculated using their respective calcium traces.  \n",
        "\n",
        "You typically will set `rf` and `stride` infrequently, so it is mainly `K`, `gSig`, and `merge_thr` that you will tweak when analyzing a given session. Note these are not the *only* important parameters. They just tend to be the *most* important, while many others tend to depend on your calcium indicator or other factors that don't vary within an experiment, and are listed above in the parameter setting section and [in the docs](https://caiman.readthedocs.io/en/master/Getting_Started.html#parameters).\n",
        "\n",
        "It is useful to know the original values of the key parameters, and we can access them using the helper function `key_params()` (we include `gSiz` in this list just so that if you tweak `gSig`, you will remember to change `gSiz` as well):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KN0O1M1ad9A5"
      },
      "outputs": [],
      "source": [
        "print(f\"Initial key params:\\n{key_params(cnmf_model)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRjgRP1Qd9A5"
      },
      "source": [
        "### How to pick parameters\n",
        "How do you determine what values to use for the key parameters with your own data? The previous section provided some guidelines on how to select them via inspection of your data. Basically: look at your movie, or a summary image for your movie, and pick values close to those suggested by the guidelines above. There is typically no *perfect* value, and there will usually be some trial-and-error involved: luckily the models are fairly robust to small changes in their values.\n",
        "\n",
        "It is helpful to use `view_quilt()` function to see if our critical spatial parameters are in the right ballpark (note we recommend running this viewer in interactive qt mode so you can interact with it and get a better feel for the parameters):"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mpld3"
      ],
      "metadata": {
        "id": "J2PSt5_g5YsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate stride and overlap from parameters\n",
        "cnmf_patch_width = cnmf_model.params.patch['rf']*2 + 1\n",
        "cnmf_patch_overlap = cnmf_model.params.patch['stride'] + 1\n",
        "cnmf_patch_stride = cnmf_patch_width - cnmf_patch_overlap\n",
        "print(f'Patch width: {cnmf_patch_width} , Stride: {cnmf_patch_stride}, Overlap: {cnmf_patch_overlap}');"
      ],
      "metadata": {
        "id": "NAcJOJI9DLHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, patch_ax = plt.subplots(figsize=(6,6))\n",
        "patch_ax = view_quilt(correlation_image,\n",
        "                      cnmf_patch_stride,\n",
        "                      cnmf_patch_overlap,\n",
        "                      vmin=0.1,\n",
        "                      vmax=0.7,\n",
        "                      alpha=0.4,\n",
        "                      color='white',\n",
        "                      figsize=(6,6),\n",
        "                      ax=patch_ax);\n",
        "mpld3.plugins.connect(fig, mpld3.plugins.Zoom())\n",
        "mpld3.display()"
      ],
      "metadata": {
        "id": "fUSgnU3A4mVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkjKoYtFd9A5"
      },
      "source": [
        "#### Evaluate spatial parameters using the quilt plot\n",
        "- Is the patch width at least three times the width of a neuron? Yes\n",
        "- Do individual neurons fit in the overlap region (`stride`)? No, current value of 6 is a bit low, so let's bump `stride` to 9 or 10.\n",
        "- When in interactive mode, you can zoom and inspect the average width of each neuron in pixels. Is `gSig` about half that? Yes. Each neuron is about 6-8 pixels wide, and `gSig` is 4.\n",
        "- For `K`: how many neurons are in each patch? Remember you want an upper bound, not an average. The current value of 4 seems good.\n",
        "\n",
        "If you can't remember the current parameter values, remember you can always use the `key_params()` convenience function to print them out:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGJ2jRSgd9A5"
      },
      "outputs": [],
      "source": [
        "key_params(cnmf_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTkwzd_3d9A6"
      },
      "source": [
        "The only parameter we are going to change is `stride`. To *change* parameters, Caiman has a built-in `change_params()` function, and you can iteratively go through the following cell, tweaking params until you are happy with them based on the `rf/stride/K/gSig` combination for your data.\n",
        "\n",
        "Note while you can just change the parameter and keep going, we recommend inspecting the patches that will result using `plot_patches()`, as there can be counterintuitive edge effects with patches that yield a lot of redundancy of computation with certain combinations of `rf` and `stride`, so it is helpful to inspect the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J18UX1gWd9A6"
      },
      "outputs": [],
      "source": [
        "rf_new = rf\n",
        "stride_new = 10\n",
        "gsig_new = gSig # unchanged\n",
        "gsiz_new = gsig_new*2 + 1\n",
        "k_new = K       # unchanged (for demo data)\n",
        "merge_thr_new = merge_thr  # unchanged\n",
        "\n",
        "print(f\"Before changing:\\n{key_params(cnmf_model)}\")\n",
        "cnmf_new_params = {'rf': rf_new,\n",
        "                   'stride': stride_new,\n",
        "                   'gSig': gsig_new,\n",
        "                   'gSiz': gsiz_new,\n",
        "                   'K': k_new,\n",
        "                   'merge_thr': merge_thr_new}\n",
        "cnmf_model.params.change_params(params_dict=cnmf_new_params)\n",
        "\n",
        "# calculate stride and overlap from parameters\n",
        "cnmf_overlap = cnmf_model.params.patch['stride'] + 1\n",
        "cnmf_patch_width = cnmf_model.params.patch['rf']*2 + 1\n",
        "cnmf_stride = cnmf_patch_width - cnmf_overlap\n",
        "\n",
        "print(f\"\\nAfter changing:\\n{key_params(cnmf_model)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoBv_9hqOxQr"
      },
      "outputs": [],
      "source": [
        "# plot the new  patches\n",
        "fig, patch_ax=plt.subplots(figsize=(6,6))\n",
        "patch_ax = view_quilt(correlation_image,\n",
        "                      cnmf_stride,\n",
        "                      cnmf_overlap,\n",
        "                      vmin=0.2,\n",
        "                      #vmax=0.7,\n",
        "                      alpha=0.4,\n",
        "                      color='white',\n",
        "                      figsize=(6,6),\n",
        "                      ax=patch_ax);\n",
        "mpld3.plugins.connect(fig, mpld3.plugins.Zoom())\n",
        "mpld3.display()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYzsx-0od9A6"
      },
      "source": [
        "The new parameters look good, but ultimately the only way to really know is to fit the model with the parameters and see how it does: often you have to iteratively search a bit in parameter space. It's time to run CNMF! Note if you are ever unhappy with your parameters you can always run through an iterative process like the one above to improve them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plRSkuySd9A6"
      },
      "source": [
        "> <h2 style=\"margin-top: 0;\">Mesmerize your search!</h2>  \n",
        "It is not always efficient to cobble together iterative parameter search from scratch, especially if you end up needing to do a grid search in a large parameter space. <a href=https://github.com/nel-lab/mesmerize-core>Mesmerize</a> is a great package built to make parameter exploration in Caiman faster and more convenient: it keeps track of all your different results, and includes powerful GPU-based tools (based on the <a href=\"https://github.com/fastplotlib/fastplotlib\">fastplotlib library</a>) for visualization of results in Jupyter notebooks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCQLD4hNd9A6"
      },
      "source": [
        "### Run CNMF\n",
        "Now that we are happy with our parameters, let's run the cnmf algorithm using the `fit()` method. With the LINdoscope sample data it takes about 2 minutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SyEeZQ1d9A6"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "cnmf_fit = cnmf_model.fit(images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCLuDbmjd9A6"
      },
      "source": [
        "### Inspect the initial estimates\n",
        "Briefly inspect the results by plotting contours of identified components using `plot_contours_nb()`.\n",
        "\n",
        "You can interactively explore this plot in your notebook with the help of the buttons on the right-hand-side of the plot (it was made using the [Bokeh](https://bokeh.org/) library). They let you zoom, pan, reset, or save the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uFlxBhFd9A6"
      },
      "outputs": [],
      "source": [
        "cnmf_fit.estimates.plot_contours_nb(img=correlation_image, cmap='gray');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsghdiKFd9A6"
      },
      "source": [
        "Note this is just an initial result, which will contain many false positives, which is to be expected. The main concern to watch for here is whether you have lots of false *negatives* (has the algorithm missed neurons?). False negatives are hard to fix later, so if you have an unacceptable number, be sure to go back and re-run CNMF with new parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUQFvCu0d9A7"
      },
      "source": [
        "### Re-run (seeded) CNMF  on the full field of view\n",
        "It is typically helpful to refine the initial estimates by re-running the CNMF algorithm seeded just on the spatial estimates from the previous step using the `refit()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ijWDhu0d9A7"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "cnmf_refit = cnmf_fit.refit(images, dview=cluster)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfNmCyztd9A7"
      },
      "source": [
        "The spatial contours of the new estimates should now look cleaner and more canonically neuronal in shape:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EC3sexsJd9A7"
      },
      "outputs": [],
      "source": [
        "cnmf_refit.estimates.plot_contours_nb(img=correlation_image);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jqk2GKPdd9A7"
      },
      "source": [
        "While the estimates look better, there is still garbage there (false positives). The next **component evaluation** stage of the pipeline will evaluate the initial set of estimates, and remove the bad ones. But first, let's disuss the estimates object."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAQ6-ouSd9A7"
      },
      "source": [
        "# The estimates class\n",
        "The main point of the CNMF algorithm was to perform source separation, to extract the spatial footprints and temporal traces of calcium activity from neurons that were hiding in the raw data. This information, and many useful methods for visualization and analysis, are contained in Caiman's `Estimates` class. An `estimates` object was generated as an attribute of your CNMF model after running `fit()` or `refit()` (e.g., you can find it in `cnmf_refit.estimates`).\n",
        "\n",
        "The rest of this notebook, from component evaluation to calculation of DFoF, is effectively an exploration of the properties and methods of the `Estimates` class. Because of its centrality, it is crucial to understand some of the basic attributes of `estimates` when working with Caiman. The most important estimates generated are:\n",
        "\n",
        "    C: denoised calcium traces (num components x num frames) -- the temporal traces\n",
        "    A: spatial components (num pixels x num components) - the spatial footprints\n",
        "    YrA: residual for each calcium trace (num components x num frames)\n",
        "    S: spike count estimate from deconvolution, if used (num components x num frames)\n",
        "    F_dff: deltaF/F -- detrended and normalized raw calcium traces (num components x num frames)\n",
        "\n",
        "To recover raw calcium traces, you can add together the denoised calcium traces and their residuals (`C + YrA`). Note also the `F_dff` calculation is not done automatically, so when you run `fit()/refit()` this field will initially be `None`. We show how to calculate it below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9krUqhKUd9A7"
      },
      "outputs": [],
      "source": [
        "# see shape of A and C\n",
        "cnmf_refit.estimates.A.shape, cnmf_refit.estimates.C.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQq5cUc8d9A8"
      },
      "source": [
        ">  <h2 style=\"margin-top: 0;\">More on Estimates</h2>  \n",
        "\n",
        "> The estimates object contains a great deal of information. The attributes are discussed in more detail <a href=\"https://caiman.readthedocs.io/en/latest/Getting_Started.html#result-interpretation\">in the documentation</a>, but you might also find exploring the <a href=\"https://github.com/flatironinstitute/CaImAn/blob/main/caiman/source_extraction/cnmf/estimates.py\">source code</a> rewarding. For instance, while most users initially care about the extracted calcium signals <em>C</em> and spatial footprints <em>A</em>, the <b>background model</b> is also very important. The background model is included in the estimate in fields <em>b</em> and <em>f</em> (which correspond to the spatial and temporal components of the low-rank background model, respectively). We discuss the background model more below.\n",
        ">\n",
        "> Also, we realize that attribute names like <em>A</em> are not very informative or Pythonic. These names are rooted in mathematical conventions from the original papers in the literature, but we may add aliases that are more transparent in the future to make things more readable/understandable.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USEb67Y-d9A8"
      },
      "source": [
        "# Component Evaluation\n",
        "As already mentioned, the initial estimates produced by CNMF creates many spurious components. Our next step is to do some some quality control, cutting out some of the bad estimates to arrive at our final set of estimates:\n",
        "\n",
        "![component evaluation image](https://raw.githubusercontent.com/EricThomson/image_sandbox/main/images/evaluation_workflow.jpg)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvUoXITod9A8"
      },
      "source": [
        "We will evaluate each component by applying the `evaluate_components()` method. The criteria Caiman uses to evaluate components are:  \n",
        "\n",
        "- **Signal to noise ratio (SNR)**: a baseline noise estimate is extraced for each raw calcium trace, and SNR during calcium transients is calculated relative to this baseline. It is stored in `estimates.SNR_comp`. Those components with high SNR are higher quality, and less likely to be false positives.\n",
        "- **Spatial correlation**: the extracted spatial footprints in `estimates.A` should be highly correlated with the fluctuations of activity in the raw movie, at least on those frames when that component is active. The correlation coefficient between an extracted spatial component and calcium fluctuations in a region of the raw data file are stored in `estimates.r_values`.\n",
        "- **CNN confidence**: Each spatial component in `estimates.A` is passed through a CNN-based classifier that produces a confidence value between 0 and 1 that the shape is a real neuron. These are stored in `estimates.cnn_preds`.\n",
        "\n",
        "The first two criteria are illustrated schematically here (see also Figure 2 of <a href=\"https://elifesciences.org/articles/38173\">the Caiman paper</a>):\n",
        "\n",
        "![component evaluation image](https://raw.githubusercontent.com/EricThomson/image_sandbox/main/images/component_evaluation.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uP4E7-oid9A8"
      },
      "source": [
        "The `evaluate_components()` method uses the above criteria to sort components into accepted and rejected components. For each criterion, there is a threshold value in `quality` field of the parameters object (the thresholds are `min_SNR`, `rval_thr`, and `min_cnn_thr`). If a unit is below *all* of those threshold values, it will be rejected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6yPKCwUd9A8"
      },
      "outputs": [],
      "source": [
        "print(\"Thresholds to be used for evaluate_components():\")\n",
        "print(f\"min_SNR = {cnmf_refit.params.quality['min_SNR']}\")\n",
        "print(f\"rval_thr = {cnmf_refit.params.quality['rval_thr']}\")\n",
        "print(f\"min_cnn_thr = {cnmf_refit.params.quality['min_cnn_thr']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you print all parameters of the quality field, you can see that besides `min_SNR`, `rval_thresh` and `min_cnn_thr` there is also `SNR_lowest`, `cnn_lowest` and `rval_lowest`. Here the components get rejected when the value in just one of these measures is lower. Let's keep the default values for now."
      ],
      "metadata": {
        "id": "wRXJAPob66k4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(cnmf_refit.params.quality)"
      ],
      "metadata": {
        "id": "Tg7Cczur6s-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPOLQEG-d9A8"
      },
      "source": [
        "Run `estimates.evaluate_components()` to sort components into accepted/rejected. Note that this can take a few minutes if you have a very large number of components:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9t1XVge1d9A8"
      },
      "outputs": [],
      "source": [
        "cnmf_refit.estimates.evaluate_components(images, cnmf_refit.params, dview=cluster);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgpF6pXLd9A9"
      },
      "source": [
        "This method filled in two arrays in the `estimates` class: `idx_components` (accepted) and `idx_components_bad` (rejected)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-W0Vwijd9A9"
      },
      "outputs": [],
      "source": [
        "print(f\"Num accepted/rejected: {len(cnmf_refit.estimates.idx_components)}, {len(cnmf_refit.estimates.idx_components_bad)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnEeLBfid9A9"
      },
      "source": [
        "> <h2 style=\"margin-top: 0;\">More on component evaluation</h2>  \n",
        "\n",
        "> In practice, SNR is the most important evaluation factor. The spatial correlation factors are less important. In particular, the CNN for spatial evaluation may be inaccurate if your neural components are not \"canonically\" shaped somata.\n",
        ">\n",
        "> You may have noticed from the description above, that when running <em>evaluate_components()</em> the three evaluation thresholds are appied <em>inclusively</em>: if a component is above <em>any</em> of the thresholds, it will pass muster. This was found in practice to be reasonable (e.g., a low SNR component that is very strongly neuronally shaped tends to not be an accident: it is just a very low SNR neuron). However, there is a second set of <b>absolute</b> threshold parameters  set for each criterion. If a component is <em>below</em> this absolute threshold for any of the evaluation parameters, it will be discarded: these are the <em>SNR_lowest</em>, <em>rval_lowest</em>, and <em>cnn_lowest</em>, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpUTMSPkd9A9"
      },
      "source": [
        "## Visualizing results\n",
        "We've sorted our components into accepted/rejected, so it's time to start looking at the results! Caiman provides many built-in `estimates` methods for visualizaing results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4G7yaLud9A9"
      },
      "source": [
        "### All contours\n",
        "We have already used `plot_contours_nb()`, but if we provide it the `idx` keyword it will split the view into accepted and rejected components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXRGy_Wwd9A9"
      },
      "outputs": [],
      "source": [
        "cnmf_refit.estimates.plot_contours_nb(img=correlation_image, idx=cnmf_refit.estimates.idx_components);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ha4PPfuid9A-"
      },
      "source": [
        "### View individual spatial/temporal components\n",
        "One of the most useful visualization tools is `nb_view_components()`, which lets you scroll through individiual spatial and temporal components (A/C). This tool also conveniently displays the values of the three evaluation criteria for each component. This can be useful if you feel you need to change your evaluation criteria and re-run `evaluate_components()`. Perhaps you have too many false negatives and want to lower your SNR threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBjy1VPBd9A-"
      },
      "outputs": [],
      "source": [
        "# view accepted components\n",
        "cnmf_refit.estimates.nb_view_components(img=correlation_image,\n",
        "                                        idx=cnmf_refit.estimates.idx_components,\n",
        "                                        cmap='gray',\n",
        "                                        thr=0.95,\n",
        "                                        #denoised_color='red',\n",
        "                                        );"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRHH6FfWd9A-"
      },
      "source": [
        "The above shows the raw traces (`C+YrA`) by default, but you can superimpose the denoised traces from `C` if you add a color to the `denoised_color` parameter. As always in Jupyter, if you are unsure how a method works, you can enter `cnmf_refit.estimates.nb_view_components?` in a new cell to get the documentation for the method.\n",
        "\n",
        "We can also view the rejected compoonents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgVeVZ4ld9A-"
      },
      "outputs": [],
      "source": [
        "# rejected components\n",
        "if len(cnmf_refit.estimates.idx_components_bad) > 0:\n",
        "    cnmf_refit.estimates.nb_view_components(img=correlation_image,\n",
        "                                            idx=cnmf_refit.estimates.idx_components_bad,\n",
        "                                            cmap='gray',\n",
        "                                            thr=0.95,\n",
        "                                            denoised_color='red')\n",
        "else:\n",
        "    print(\"No components were rejected.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omVo5CEyd9A-"
      },
      "source": [
        "> One legacy from Matlab with these plotters is that they use one-based indexing when showing `Neuron number`. This can make it confusing when comparing to your own plotting results which will use zero-based indexing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmoYRjA-d9A-"
      },
      "source": [
        "### Building your own visualizations\n",
        "> This is a slightly more advanced section that you can safely skip your first time through.\n",
        "\n",
        "There are many custom visualizations you can build yourself based on the estimates generated from Caiman. For example, if you wanted to plot the spatial footprint of a neuron with the contour superimposed, this information is already contained in `estimates`.  The contours of the spatial footprints are in `estimates.coordinates`, which is a list of dictionaries corresponding to each component. The dictionary includes a `coordinates` field that contains the x,y coordinates of the contour. Here we'll show how to plot this superimposed on the corresponding spatial footprint in `A`.\n",
        "\n",
        "The following extracts all of the contours of the accepted components into a list from the estimates (it puts them in a list because they are not all the same size):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5GrT0iad9A_"
      },
      "outputs": [],
      "source": [
        "idx_accepted = cnmf_refit.estimates.idx_components\n",
        "all_contour_coords = [cnmf_refit.estimates.coordinates[idx]['coordinates'] for idx in idx_accepted]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoPUZAa1d9A_"
      },
      "source": [
        "Each footprint in `A` is stored as a compressed sparse column array. We can convert it to a dense array with `toarray()`, and plot the contour and footprint together:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmvVQ-pJd9BA"
      },
      "outputs": [],
      "source": [
        "idx_to_plot = 30\n",
        "component_contour = all_contour_coords[idx_to_plot]\n",
        "component_footprint = np.reshape(cnmf_refit.estimates.A[:, idx_accepted[idx_to_plot]].toarray(), dims, order='F')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLIlNNByd9BA"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.imshow(component_footprint, cmap='gray');\n",
        "ax.plot(component_contour[:, 0],\n",
        "         component_contour[:, 1],\n",
        "         color='pink',\n",
        "         linewidth=2)\n",
        "ax.set_title(f'Footprint/Contour {idx_to_plot}');\n",
        "mpld3.plugins.connect(fig, mpld3.plugins.Zoom())\n",
        "mpld3.display()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhXuxTP3d9BB"
      },
      "source": [
        "# A few final things\n",
        "We have extracted the calcium traces `C`, spatial footprints `A`, and estimated spike counts `S`, which is the main goal with CNMF. But there are a few important things remaining."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWixPnAWd9BB"
      },
      "source": [
        "## Extract $\\Delta F/F$ values\n",
        "So far our calcium traces are in arbitrary units. In the literature, it is common to report the calcium fluorescence relative to some baseline value. In Caiman the baseline is a running percentile calculated over a `frames_window` moving window. You can calculate $\\Delta F/F$ using raw traces or the denoised traces in C (this is toggled using the `use_residuals` argument):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FFcY0q-d9BB"
      },
      "outputs": [],
      "source": [
        "if cnmf_refit.estimates.F_dff is None:\n",
        "    print('Calculating estimates.F_dff')\n",
        "    cnmf_refit.estimates.detrend_df_f(quantileMin=8,\n",
        "                                      frames_window=250,\n",
        "                                      use_residuals=False);  # use denoised data\n",
        "else:\n",
        "    print(\"estimates.F_dff already defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mz2Y86txd9BB"
      },
      "source": [
        "The estimates object will now have a `F_dff` field, which makes it easier to compare traces across neurons/sessions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxFDnu76d9BB"
      },
      "source": [
        "## Select only accepted components (optional)\n",
        "If you want to discard rejected components (`estimates.idx_components_bad`) from the `estimates` field, you can run  `select_components()`. This can be useful if you are sure you only want to focus on the accepted components for downstream analysis (e.g., to share final results with colleagues, for instance).\n",
        "\n",
        "> <h4 style=\"margin-top: 0;\">Warning: select_components() is a destructive operation</h4>  \n",
        "If you run this command, the rejected components will be removed from your <em>estimates</em> field. If you think you might want them later, you can set the <em>save_discarded_components</em> parameter to <em>True</em>. That way you can retrieve them later with the <em>restore_discarded_components()</em> method.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1vqUaiPd9BB"
      },
      "outputs": [],
      "source": [
        "cnmf_refit.estimates.select_components(use_object=True);"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `use_object` parameter specifies that we want to select the accepted components in `estimates.idx_components` (and remove the components in `idx_components_bad`). Alternatively, you could also specify the indices of the components using an `idx_components` parameter)."
      ],
      "metadata": {
        "id": "OelF4shuGy6V"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tElCv3_Bd9BB"
      },
      "source": [
        "## Display final results\n",
        "View the final refined set of remaining components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSS0u5gOd9BC"
      },
      "outputs": [],
      "source": [
        "cnmf_refit.estimates.nb_view_components(img=correlation_image,\n",
        "                                        denoised_color='red',\n",
        "                                        cmap='gray');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-h7lgP-d9BC"
      },
      "source": [
        "## View different result movies\n",
        "### An aside on the underlying model\n",
        "To understand the next two visualizations, we need to discuss the model used by Caiman a little bit. In broad terms, it is relatively simple: the CNMF algorithm models the original movie as a sum of *neural activity*, *background activity*, and *noise* (or *residual*):\n",
        "\n",
        "    original_movie = neural_activity + background + residual\n",
        "    \n",
        "In this model, `neural_activity` is the product of the matrices `AC`, the spatial and temporal components we have been exploring (`estimates.A` and `estimates.C`). It is our model of the neural bits that we care about.\n",
        "\n",
        "`background` is the model's representation of all the background activity our movie that we wish wasn't there -- this includes stray fluorescence from the neuropil and out-of-focus neural components. This background model is also broken up into spatial and temporal components, which are in `estimates.b` (background) and `estimates.f` (fluctuations), respectively. These components are positive, but otherwise unconstrained, low-dimension (typically 1 or 2 rank -- this is controlled by the `gnb` parameter) models that capture the spatially very large-scale background fluctuating activity in the movie.\n",
        "\n",
        "The \"noise\", or residual term, is by definition, everything else not captured by the model:\n",
        "\n",
        "    residual = original_movie - neural_activity - background"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6a_W0Wjd9BC"
      },
      "source": [
        "### View denoised movie\n",
        "We can rearrange the equations above to yield a \"denoised\" movie, which is just the original movie with the residual removed:\n",
        "\n",
        "    denoised_movie = original_movie - residual = neural_activity + background\n",
        "    \n",
        "Plugging in appropriate terms from the models of neural activity and background activity (`AC` and `bf`) yields:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fPNbeIOd9BC"
      },
      "outputs": [],
      "source": [
        "# reconstruct denoised movie\n",
        "neural_activity = cnmf_refit.estimates.A @ cnmf_refit.estimates.C  # AC\n",
        "background = cnmf_refit.estimates.b @ cnmf_refit.estimates.f  # bf\n",
        "denoised_movie = neural_activity + background  # AC + bf\n",
        "\n",
        "# build denoised movie object\n",
        "denoised_movie = cm.movie(denoised_movie).reshape(dims + (-1,), order='F').transpose([2, 0, 1])\n",
        "denoised_movie = denoised_movie.resize(fz=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwqqg4xWd9BD"
      },
      "source": [
        "View the denoised movie object (with temporal subsampling)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "min_ = np.percentile(denoised_movie, 0.0001)\n",
        "max_ = np.percentile(denoised_movie, 99.999)\n",
        "denoised_handle = np.array((denoised_movie - min_)/(max_ - min_)*255, dtype='uint8')  # normalize 0-255 uint8 movie"
      ],
      "metadata": {
        "id": "0s2sKtBfJA26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQYZ0Qa4RPCq"
      },
      "outputs": [],
      "source": [
        "# View in colab\n",
        "imageio.mimwrite('/root/caiman_data/denoised_movie.mp4', denoised_handle, fps = 30,  quality=8)\n",
        "display_movie = True\n",
        "if display_movie:\n",
        "    mp4 = open('/root/caiman_data/denoised_movie.mp4','rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "    display(HTML(\"\"\"\n",
        "    <video width=400 controls>\n",
        "    <source src=\"%s\" type=\"video/mp4\">\n",
        "    </video>\n",
        "    \"\"\" % data_url))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note there are a **lot more** things in the main demo notebook at the github repo. For this limited demo we are just giving a flavor of how things will work when you run locally!"
      ],
      "metadata": {
        "id": "uLEVzAk4H0Au"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "295S3Ulrd9BE"
      },
      "source": [
        "# Clean up open resources\n",
        "We have a few resources we have left open we should take care of.\n",
        "\n",
        "## Shut down cluster\n",
        "To free up processing resources, let's shut down the cluster in case it is still open."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTPYKZAtd9BE"
      },
      "outputs": [],
      "source": [
        "cm.stop_server(dview=cluster)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOQlxfCUd9BE"
      },
      "source": [
        "## Shut down logger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0EwOcF_d9BE"
      },
      "outputs": [],
      "source": [
        "# Shut off logger\n",
        "logging.shutdown()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HdAO7OctGVZc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}